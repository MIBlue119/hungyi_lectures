<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Hung-yi Lee's Lectures(台大李宏毅老師系列課程)</title>
    <style>
        body {
            font-family: sans-serif;
            font-size: 18px;
            color: #111;
            padding: 0 0 1em 0;
        }
        .l {
          color: #050;
        }
        .s {
            display: inline-block;
        }
        .e {
            display: inline-block;
        }
        .t {
            display: inline-block;
        }
    </style>
  </head>
  <body>
    <a href="../../index.html">back to index</a>
    <h2>Network Compression (6/6)</h2><a href=https://www.youtube.com/watch?v=f0rOMyZSZi4><img src=https://i.ytimg.com/vi_webp/f0rOMyZSZi4/hqdefault.webp></a><br>
    
    <div class="c">
        <a class="l" href="#00:01.000" id=00:01.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=1">00:01.000</a></div>
        <div class="t">那這邊再更直觀的幫大家解釋一下說,原來的convolution跟現在這個describe separable的convolution有什麼樣的關係。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:13.280" id=00:13.280>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=13">00:13.280</a></div>
        <div class="t">在原來的convolution裡面,你input這樣子的東西,output這樣子的東西,那左上角的這個值是怎麼被算出來的呢?左上角這個值是說,我們有一個3乘以3乘以2的filter,我們把這個3乘以3乘以2的filter放在這個input的feature map的左上角跟input feature map的左上角的這3乘以3乘以2的值做搭發搭,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:41.200" id=00:41.200>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=41">00:41.200</a></div>
        <div class="t">然後你就得到左上角這個值,你把你的filter放在這邊,做搭發搭以後就得到這個值,所以你可以想像說你的filter就是吃18個輸入,然後得到一個輸出,把它放在左上角。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:59.120" id=00:59.120>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=59">00:59.120</a></div>
        <div class="t">同理,比如說,黃色的這個matrix,它左下角的值是怎麼來的呢?你是把另外一個filter放在左下角,這個filter也是吃18個值當作input,得到一個輸出,就是一般的convolution。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:15.120" id=01:15.120>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=75">01:15.120</a></div>
        <div class="t">好,那我們剛才講的那個dashwise separable的convolution,跟一般的convolution有什麼不同呢?dashwise的convolution,左上角的值是怎麼來的呢?這個左上角的值是來自於一個中間產物。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:29.840" id=01:29.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=89">01:29.840</a></div>
        <div class="t">所以其實這個dashwise的這個separable的convolution,它的概念跟我們最開始講的那個,我們剛才講說如果你要讓fully connected v-forward的值,the network的值變出來,是中間就插一個linear的hidden layer,就可以讓它的參數量變少。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:47.280" id=01:47.280>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=107">01:47.280</a></div>
        <div class="t">那其實這邊的概念是很像,我們是在中間再插一個東西,我們在中間再插一個feature map,那這個feature map它的channel跟input的feature map的channel是一樣的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:00.240" id=02:00.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=120">02:00.240</a></div>
        <div class="t">那這個紅色的左上角的值是怎麼來的呢?它是從這個中間產物的左上角,根據1x1的feature來的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:16.240" id=02:16.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=136">02:16.240</a></div>
        <div class="t">那這個中間產物的左上角的值又是怎麼來的呢?這個中間產物是根據我們剛才講的這個沒有深度的feature得到的,每一個feature只處理自己的channel,它完全不管其他的channel。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:31.600" id=02:31.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=151">02:31.600</a></div>
        <div class="t">所以今天你可以想成是,我們在第一個步驟的時候,我們input如果是有兩個channel的話,那我們就有兩個feature,這兩個feature它們各自只處理9個input。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:50.160" id=02:50.160>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=170">02:50.160</a></div>
        <div class="t">接下來它們的output會有另外一個紅色的neural,把它們兩個的output吃進去,再得到最終的輸出。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:58.000" id=02:58.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=178">02:58.000</a></div>
        <div class="t">在desk-wide separable convolution裡面,每一個feature其實可以看作是拆成兩層,它中間還有一個different layer,我們把原來的feature吃18個input得到一個output,拆解成這18個input拆成9個input跟9個input,分別用兩個neural來處理,最終再把這兩個neural的輸出丟給紅色這個neural,再得到最終的輸出。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:20.800" id=03:20.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=200">03:20.800</a></div>
        <div class="t">如果我們看黃色的這個matrix的左下角,黃色這個matrix的左下角是怎麼來的呢?黃色這個matrix的左下角是從中間產物的左下角來的,中間產物的左下角是從本來feature map的左下角來的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:38.080" id=03:38.080>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=218">03:38.080</a></div>
        <div class="t">我們現在在處理這個feature map的時候,我們用的都是這兩個藍色的neural,所以你把同樣這兩個藍色不同input的neural的輸出,再丟給黃色的neural,再得到輸出。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:52.640" id=03:52.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=232">03:52.640</a></div>
        <div class="t">所以你會發現說在dense white separable的convolution裡面,不同的feature它們前面的參數,它們有部分的參數是共用的。就在dense white separable convolution裡面,我們把原來的feature拆解成兩層,而它們的第一層參數是共用的,在第二層採用不同的參數。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:16.000" id=04:16.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=256">04:16.000</a></div>
        <div class="t">那這就是為什麼dense white convolution它可以用比較少的參數,但是可以做到類似的事情。所以不同feature間,我們讓它共用同樣的參數。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:27.520" id=04:27.520>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=267">04:27.520</a></div>
        <div class="t">好,那我們接下來在理論上算一下,有用dense white separable convolution跟一般的convolution,它們的參數量大概有什麼樣的差距。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:41.040" id=04:41.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=281">04:41.040</a></div>
        <div class="t">我們現在假設i是input channel的數目,o是output channel的數目。在剛才的例子裡面,我們設i等於2,o等於4,k是feature的size,那我們這邊的例子是設3乘以3。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:01.520" id=05:01.520>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=301">05:01.520</a></div>
        <div class="t">那在一般的convolution裡面,每一個feature的參數量都是k乘以k乘以i,每一個feature都是立體的,你的是curl size是k乘以k,但它是有深度的,它的深度是input channel的數目,也就是i。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:17.520" id=05:17.520>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=317">05:17.520</a></div>
        <div class="t">那你有幾個output channel,那你就應該要有幾個feature,你有o個output channel,你就應該有o個feature,所以最終一般的convolution,它需要的參數量是k乘以k乘以i,這是每一個feature的參數量,再乘上我們總共需要的o個feature。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:36.720" id=05:36.720>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=336">05:36.720</a></div>
        <div class="t">那接下來呢,我們再看described separable的convolution,這個新的convolution,這個簡化後的convolution,它需要多少的參數。在第一個步驟,我們說在第一個步驟,我們的feature是沒有深度的,每一個feature只自己管自己的事。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:52.720" id=05:52.720>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=352">05:52.720</a></div>
        <div class="t">沒有深度的feature,它的參數是k乘以k,如果input有i個channel,那我們需要i個feature,因為有i個channel,所以我們需要i個feature,每一個人去管一個channel,所以是k乘以k乘以i的參數。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:05.760" id=06:05.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=365">06:05.760</a></div>
        <div class="t">接下來在第二步,我們用的是1乘以1的feature,那每一個1乘以1的feature有多少參數呢?如果你input的channel是i的話,那每一個feature就有i個參數,output的channel有多少,我們就需要多少1乘以1的feature,所以這些1乘以1的feature的參數的總量是i乘以o。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:27.040" id=06:27.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=387">06:27.040</a></div>
        <div class="t">所以that's why separable convolution它的參數是k乘以k乘以i再加上i乘以o,那我們可以把這兩者相除,你就可以感受一下這兩種convolution它的參數的差距。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:41.120" id=06:41.120>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=401">06:41.120</a></div>
        <div class="t">所以我們把that's why separable convolution的參數放在分子的地方,把一般的convolutional的layer的參數放在分母的地方相除,那通常o很大,o是optional的數目,我們通常設個512、128,那只是蠻大的,所以我們不用無視它,我們只考慮1除以k乘以k的這項。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:01.680" id=07:01.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=421">07:01.680</a></div>
        <div class="t">那k你通常設,比如說設3,通常你feature設3乘以3是蠻常見的setup,那如果你設k設3的話,那你network的size就變成原來的1九分之一,你network的參數量就變成原來的1九分之一。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:16.880" id=07:16.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=436">07:16.880</a></div>
        <div class="t">那像這樣的設計,廣泛地被用在今天各種號稱比較小的network裡面,舉例來說squeeze net、mobile net、shuffle net還有exception,那其中我想最知名的就是mobile net,你從它的名字就知道mobile net它就是為了把network放在手機上而設計的,所以叫做mobile net。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:40.880" id=07:40.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=460">07:40.880</a></div>
        <div class="t">那最後我們要跟大家講的主題,這邊非常的簡短,叫做dynamic computation,那dynamic computation舉例來說,今天你的手機如果電量很不夠的時候,那你的network它可以自動知道我要減少運算量,先求有再求好,雖然說先求有再求好,但是也希望在現有的運算資源上可以盡量做到最好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:05.680" id=08:05.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=485">08:05.680</a></div>
        <div class="t">另外一方面,如果運算資源很充足,那麼network就使盡全力把它的結果、把它的力量發揮出來。這件事情要怎麼做呢?一個最trivial、最簡單的solution就是,你就train一把network,合成一把network,從最深的到最淺的,從參數最高的到參數最小的,然後接下來再根據你現在device的情境,根據你device現在computing power,選擇一個適當的network。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:35.040" id=08:35.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=515">08:35.040</a></div>
        <div class="t">那這樣做的壞處就是,你需要存一大堆的network,這顯然會非常佔你的儲存空間,尤其是在device上你的儲存空間不太夠,你可能沒有辦法存一打的network,來根據不同的狀況選擇不同的network。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:49.440" id=08:49.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=529">08:49.440</a></div>
        <div class="t">另外一個方法是,我們能不能夠train一個network,但是這個network它可以,如果你給它不同的限制,它都可以做不同的應對。舉例來說,我們可以在一個network的每一個中間的hidden layer都拉出來,就做classify。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:12.800" id=09:12.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=552">09:12.800</a></div>
        <div class="t">所以這個network架構可能長得是這樣子,就是你一般的network有很多層,一直要到最後一層你才會知道結果,但我們能不能夠訓練一些classify,這些classify是把network的中間層拿出來,根據network的中間層就決定結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:30.400" id=09:30.400>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=570">09:30.400</a></div>
        <div class="t">那今天在實作的時候,你只要根據,今天在測試在使用這些network的時候,你就可以根據你的運算資源調整說,你現在到底是要跑完第一層就得到結果,還是要跑完一二層才得到結果,你就可以自由地調整你network需要的運算量。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:48.800" id=09:48.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=588">09:48.800</a></div>
        <div class="t">但是光直接這麼做是不太好的,為什麼?因為你可以想像說,一個network前面幾個layer,它的builder抽的東西都只是很簡單的pattern,一直要到後面的layer,你的CNN抽出來的東西才是比較有用的feature。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:06.400" id=10:06.400>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=606">10:06.400</a></div>
        <div class="t">前面幾個layer抽出來的只是很簡單的pattern,那你前面幾個layer接出來的那些classify,它的performance可能不是太好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:13.840" id=10:13.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=613">10:13.840</a></div>
        <div class="t">那在實作上,確實是如此。那這邊我們先忽視NSDnet,也就是黑色這一條線。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:20.760" id=10:20.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=620">10:20.760</a></div>
        <div class="t">這個NSDnet是proposed專門處理這種dynamic computing的network,所以我們就忽視它。我們只看紅色的DenseNet跟藍色的ResNet。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:30.640" id=10:30.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=630">10:30.640</a></div>
        <div class="t">縱軸是正確率,橫軸是每一個階段,從最淺的地方一直到最深的地方,當我們把classify拉出來的時候,那些classify可以得到的正確率。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:46.320" id=10:46.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=646">10:46.320</a></div>
        <div class="t">你發現說,如果你在很淺的地方就把classify拉出來,你的classify吃的是很接近input的hidden layer的output,那些classify的結果都是不太好的,因為在input的時候,那些builder,因為在input的時候,你的CNN抽出來的pattern是很簡單的,所以那些classify沒有辦法做出太厲害的事情。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:07.680" id=11:07.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=667">11:07.680</a></div>
        <div class="t">還有另外一個問題是,你今天在中間加了一些classify,這些classify他們是跟這個networkjointly trained的,這些classify會傷害到原來network的performance,為什麼這些classify會傷害到原來network的performance呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:27.360" id=11:27.360>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=687">11:27.360</a></div>
        <div class="t">我們在講CNN的時候,我們有講過說,CNN比較底層的builder就是抽簡單的pattern,簡單的pattern組合起來才能夠偵測複雜的pattern,但是如果你在第一層就加了一個classify,那這個classify也會想要得到好的正確率。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:43.640" id=11:43.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=703">11:43.640</a></div>
        <div class="t">所以就是強迫說,前面的builder必須要抽一些這個classify可以用來得到正確答案的pattern,那就是破壞整個原來的CNN的佈局,就本來前面只需要抽簡單的東西,再把簡單的東西組成複雜的東西。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:59.240" id=11:59.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=719">11:59.240</a></div>
        <div class="t">但是現在有一個classify,它需要一些複雜的pattern,就會強迫第一個layer本來不該作為抽複雜的pattern,它也強迫它抽一些複雜的pattern,會破壞掉整個CNN的佈局。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:09.920" id=12:09.920>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=729">12:09.920</a></div>
        <div class="t">所以你會發現說,如果你今天加了一些classify,performance是會掉的。我們看藍色這一條線,藍色這一條線是resnet的結果,縱軸是相對的正確率,橫軸是我們今天在不同的layer加上classify。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:29.440" id=12:29.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=749">12:29.440</a></div>
        <div class="t">如果你在很接近input的地方加一個classify,整個performance就會暴跌,在比較接近output的地方加classify,影響是比較小的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:41.960" id=12:41.960>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=761">12:41.960</a></div>
        <div class="t">有沒有方法可以解決這個問題呢?有的,就這些圖其實都是來自於NSDnet這篇paper,那如果你想要知道說dynamic computing怎麼做的話,那NSDnet也許是一個你可以參考的方向,因為今天時間有限,我們就講到這邊就好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:59.000" id=12:59.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=f0rOMyZSZi4&t=779">12:59.000</a></div>
        <div class="t">我們接下來就請助教來講一下作業。</div>
    </div>
    
</body>
</html>   