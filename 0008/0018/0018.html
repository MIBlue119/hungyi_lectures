<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Hung-yi Lee's Lectures(台大李宏毅老師系列課程)</title>
    <style>
        body {
            font-family: sans-serif;
            font-size: 18px;
            color: #111;
            padding: 0 0 1em 0;
        }
        .l {
          color: #050;
        }
        .s {
            display: inline-block;
        }
        .e {
            display: inline-block;
        }
        .t {
            display: inline-block;
        }
    </style>
  </head>
  <body>
    <a href="../../index.html">back to index</a>
    <h2>Evaluation of Generative Models</h2><a href=https://www.youtube.com/watch?v=VNqOspvEKEI><img src=https://i.ytimg.com/vi_webp/VNqOspvEKEI/hqdefault.webp></a><br>
    
    <div class="c">
        <a class="l" href="#00:00.000" id=00:00.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=0">00:00.000</a></div>
        <div class="t">講evaluation,就是既然有這麼多gain的方法,如果你要比較不同的gain的方法的話,你要怎麼比較它,你要怎麼evaluate你一個方法是好還是不好呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:15.500" id=00:15.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=15">00:15.500</a></div>
        <div class="t">這邊主要的reference是出自這篇paper,這篇paper不是針對gain寫的,它是針對所有generative的model寫的,所以這邊講的東西其實不是只能apply在gain上面,可以apply在各種不同的generative的model上面。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:33.500" id=00:33.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=33">00:33.500</a></div>
        <div class="t">最常衡量一個generative model好壞的方法,就是計算這個generative model,generate real data的likelihood,這邊藍色的這些點代表的是real的data,是真正的data。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:50.500" id=00:50.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=50">00:50.500</a></div>
        <div class="t">當然必須要強調一下,這個generative model在訓練的時候,它也會開一堆real的data,但是在計算likelihood,在衡量一個generative好壞的時候,你用那個real data不可以是generate看過的data,我想這個大家應該都有這樣的概念。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:06.000" id=01:06.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=66">01:06.000</a></div>
        <div class="t">如果你今天可以計算generator產生某一筆data的機率的話,那你就可以計算這個generator產生這一把real data的likelihood。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:22.000" id=01:22.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=82">01:22.000</a></div>
        <div class="t">就計算說現在generator產生這邊每一個x它的機率,這邊寫的pg of x,然後取log再相加再取平均,就得到log likelihood。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:37.000" id=01:37.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=97">01:37.000</a></div>
        <div class="t">那個generator如果它log likelihood越大,顯然它越有可能產生這些real的data,它的performance就越好,它產生的data可能就越realistic。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:45.000" id=01:45.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=105">01:45.000</a></div>
        <div class="t">但是對GaN來說,這個方法是有問題的。如果一個generator是比較簡單的model,比如說它是Gaussian mixture model,這可以算,沒有問題。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:55.000" id=01:55.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=115">01:55.000</a></div>
        <div class="t">Gaussian mixture model怎麼算likelihood,我想大家都知道。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:58.000" id=01:58.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=118">01:58.000</a></div>
        <div class="t">但如果它是GaN的話就麻煩,GaN沒辦法直接算這一項,沒辦法直接算機率,GaN它唯一能做的事情就是sample,GaN只能做sample。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:10.000" id=02:10.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=130">02:10.000</a></div>
        <div class="t">所以我們只能說我們有一個prior distribution,從這個prior distribution裡面我們sample一個vector z出來,然後把這個z丟進去,你得到一個粉紅色的星星代表一筆generated data。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:23.000" id=02:23.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=143">02:23.000</a></div>
        <div class="t">你可以generate一打data,但是你卻算不出用這個generator產生某一筆data的likelihood,因為我們通常處理的問題是在high dimensional處理這些問題。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:38.000" id=02:38.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=158">02:38.000</a></div>
        <div class="t">比如說這邊每一個point它可能是一張image,它們是一個high dimensional vector,你不管怎麼用你的generatorgenerate,你可能永遠都generate不出和你的real data一模一樣的x,所以你沒辦法算likelihood。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:52.000" id=02:52.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=172">02:52.000</a></div>
        <div class="t">怎麼辦呢?有一個方法是先用generator產生一把sample,接下來再根據這一把sample用另外一個比較簡單的distribution去approximate你的generator。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:09.000" id=03:09.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=189">03:09.000</a></div>
        <div class="t">既然generator沒辦法算likelihood,但quotient你有辦法算likelihood,那你能不能說現在generator產生這四筆data,我們就假設這四筆data是四個quotient的中心,是四個quotient的mean,然後這四個quotient都有一個我們已經事先決定好的covariance的metric,然後我們說我們這邊有一個quotient的mixture所組成的distribution,那這個quotient的mixture所組成的distribution,它就可以去approximate我們的generator。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:38.000" id=03:38.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=218">03:38.000</a></div>
        <div class="t">如果你這麼做的話,你確實就可以計算likelihood了。如果你這麼做的話,你就有一個可以計算likelihood的model,而這個可以計算likelihood的model,它可以去approximate generator,這樣你就可以approximate這個generator的likelihood。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:57.000" id=03:57.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=237">03:57.000</a></div>
        <div class="t">但是這邊有個問題就是,你說要從generator裡面sample一些data去做approximation,那到底應該sample多少data才夠呢?所以文件上有這樣一個結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:09.000" id=04:09.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=249">04:09.000</a></div>
        <div class="t">這個文件上的例子其實是一個toy example,它是一個很簡單的example,它還不是一個真正的問題,它是一個簡單的example,它說我們有一個quotient mixture model,那這個quotient mixture model我們已經知道這個quotient mixture model產生某筆data的likelihood,這個quotient mixture model它的likelihood是loglikelihood是兩百左右。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:32.000" id=04:32.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=272">04:32.000</a></div>
        <div class="t">好,那你現在從這個quotient mixture model裡面去sampledata出來,然後再去用我們剛才講的方法去estimate另外一個distribution,去approximate現在sampledata的quotient mixture model。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:47.000" id=04:47.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=287">04:47.000</a></div>
        <div class="t">那你會發現說,你要sample很多很多很多的data,比如說這邊sample到十個七十方筆的data,你estimate出來的那個approximate的model,它算出來的likelihood仍然跟原來的model有很大的差距。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:02.000" id=05:02.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=302">05:02.000</a></div>
        <div class="t">當然如果你今天sampledata夠多sampled到無窮多,最後這個紅線可能會趨近於黑線,但你要sample非常多的點,才能夠做到這件事情。那另外一個問題是,有時候你用這個方法算,算出來的結果會有點奇怪。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:21.000" id=05:21.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=321">05:21.000</a></div>
        <div class="t">舉例來說,這個也是那篇paper上面的例子,它說在某個test上面,dbn算出來的likelihood是一百三十八,gain算出來的likelihood是二二五,那你就覺得很高興,哦,gain算出來的likelihood比較高,所以它是比較好的model。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:36.000" id=05:36.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=336">05:36.000</a></div>
        <div class="t">然後真正的data算出來的likelihood是二四三,就是說你從真正的data distribution裡面去sample一些point出來,再去estimate一個model,然後再用這個model去產生這個database裡面真正的data的話,likelihood是二四三。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:56.000" id=05:56.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=356">05:56.000</a></div>
        <div class="t">那理論上,這個likelihood應該是一個upper bound,你是從真正的data裡面去sampledata出來做estimation的,你可能沒有辦法再做得更好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:06.000" id=06:06.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=366">06:06.000</a></div>
        <div class="t">但事實上呢,它做了一個k-means,就可以打爆其他方法,這個k-means的方法是說把data做一下k-means,然後把k-means的中心sample出來,當作你的distribution。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:22.000" id=06:22.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=382">06:22.000</a></div>
        <div class="t">那這個時候呢,你就會發現說,你用k-means這個方法呢,居然還比真正的distribution得到的結果還要好,所以這個方法呢,有點奇怪。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:32.000" id=06:32.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=392">06:32.000</a></div>
        <div class="t">那再更進一步來說,我們今天在做game的時候,我們通常是希望我們的game產生出來的image,假設你是做image的generation的話,它產生出來的image呢,是realistic的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:45.000" id=06:45.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=405">06:45.000</a></div>
        <div class="t">那事實上,產生出來的image是不是realistic,這件事情跟likelihood不見得是有一定的關係的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:53.000" id=06:53.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=413">06:53.000</a></div>
        <div class="t">有可能有一個model,它產生的image都是很high quality的,但是它算出來的likelihood卻很低。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:01.000" id=07:01.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=421">07:01.000</a></div>
        <div class="t">舉例來說,現在有一個generator,然後這個generator產生出來的圖呢,其實產生出來的圖都是很好,但是它產生出來的variance都很低,然後產生出來的都是兩公尺,只能產生同一個人。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:15.000" id=07:15.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=435">07:15.000</a></div>
        <div class="t">那這個時候,如果你拿它產生的這些data去estimate一個model,然後再去計算這個model產生你的testing data likelihood,你可能會發現說,產生出這些testing data的likelihood的機率很低。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:32.000" id=07:32.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=452">07:32.000</a></div>
        <div class="t">那可能假設testing data裡面根本沒有兩公尺的話呢,那你根本沒有辦法產生testing data裡面的任何一張image,就算說這個model產生出來的圖已經很好了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:43.000" id=07:43.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=463">07:43.000</a></div>
        <div class="t">而反過來說,好的likelihood,高的likelihood就一定保證好的image可以被產生嗎?其實也不是。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:53.000" id=07:53.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=473">07:53.000</a></div>
        <div class="t">低的likelihood有可能產生好的image,高的likelihood也有可能產生不好的image。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:03.000" id=08:03.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=483">08:03.000</a></div>
        <div class="t">舉例來說,假設有一個generator G1,這個generator它可以generate一堆很好的image,那我們現在可以計算這個G1的likelihood,假設G1的likelihood就是L。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:17.000" id=08:17.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=497">08:17.000</a></div>
        <div class="t">那現在呢,假設有另外一個generator G2,這個generator G2呢,它這邊有點被打到了,這個是0.01,大家有看到嗎?這個generator G2它有0.01,它有1%的狀況,它跟generator G1的behavior是一樣。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:36.000" id=08:36.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=516">08:36.000</a></div>
        <div class="t">它有1%的時候,它會generate很好的圖,但是其他99%的時候,它產生的圖都很爛,都是random。這個時候,G2的likelihood應該是多少呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:48.000" id=08:48.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=528">08:48.000</a></div>
        <div class="t">因為G2它只有1%的機率會跟G1一樣,所以你要算G2的likelihood,其實就是把G1的這個機率,這個pg of xi除以100。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:06.000" id=09:06.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=546">09:06.000</a></div>
        <div class="t">它要產生xi的機率是pg of xi,那G2只有1%的機率會跟G1一樣,所以它產生xi的機率就是pg of xi除以100。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:18.000" id=09:18.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=558">09:18.000</a></div>
        <div class="t">那你現在把這個式子,把log相除的相減,你把減log100這件事情提出去,就變成說新的這個G2呢,它的likelihood會比G1的likelihood少掉-log100。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:36.000" id=09:36.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=576">09:36.000</a></div>
        <div class="t">那-log100是多少呢?-log100其實很小,-log100才4.6。你看剛才我們舉的那幾個例子,那個likelihood的差距都是10幾以上,你likelihood差距只有4.6,其實很小。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:49.000" id=09:49.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=589">09:49.000</a></div>
        <div class="t">也就是說,如果光從likelihood來看的話,G1跟G2它的likelihood差距才4點多,聽起來好像不大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:56.000" id=09:56.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=596">09:56.000</a></div>
        <div class="t">但是實際上呢,G2它只有1%的機會會跟G2產生一樣好的input,大部分的時候它產生的都是壞的input,但它還是可以算出很高的likelihood,所以有很多奇怪的地方。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:12.000" id=10:12.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=612">10:12.000</a></div>
        <div class="t">那有一個可以來evaluate你產生的image的好壞的方法是說,拿另外一個現成的image的classifier,比如說拿一個已經train好的CNN,然後把你的model的圖呢,產生出來的圖呢,丟進這個已經train好的CNN裡面,看它的output怎樣。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:32.000" id=10:32.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=632">10:32.000</a></div>
        <div class="t">我們知道CNN的output,如果你今天train這個CNN是一個image的classifier,如果它是一個image的classifier,那它的output是一個probability distribution,input是x,y是label,假設你今天這個y是代表1000個不同的class的話呢,這個CNN的output就是一個1000維的vector。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:56.000" id=10:56.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=656">10:56.000</a></div>
        <div class="t">那今天這個distribution,這個x在不同的class上分佈的distribution,如果這個distribution的分佈越分散,就代表產生的image越差,如果分佈越集中,代表產生的image越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:11.000" id=11:11.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=671">11:11.000</a></div>
        <div class="t">因為這個CNN它訓練的目的就是要分那張image,如果它今天可以很confidence的知道你input的這個image裡面有什麼option的話,意思就是說,你現在就可以想說可能是你產生的image是非常清楚的,所以這個CNN才可以很果斷的判斷說,現在input的option是什麼東西。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:32.000" id=11:32.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=692">11:32.000</a></div>
        <div class="t">好,那一個distribution集不集中,我們知道可以用entropy來表示,所以如果這個CNN的output,它的entropy越低,代表它的產生的image可能就越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:48.000" id=11:48.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=708">11:48.000</a></div>
        <div class="t">那另外一件我們可能會想要知道的事情是,我們現在generator產生出來的影像有多diverse,我們知道在change gate的時候你會有mulcless的問題,所以我們要檢查說,現在我們的model是不是可以產生多樣化的圖案。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:08.000" id=12:08.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=728">12:08.000</a></div>
        <div class="t">所以我們現在如果有讓model產生一大堆image,X1,X2,X3,然後把每張imageX1,X2,X3丟到CNN裡面,每一個X1,X2,X3都給我們一個distribution,那我們期待什麼呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:24.000" id=12:24.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=744">12:24.000</a></div>
        <div class="t">我們期待說,如果我們把所有不同的distribution,所有不同的X所產生的distribution平均起來,這個新的distribution它應該是要非常分散。如果它非常分散意味著說,每次我丟不同的X的時候,我output的distribution都很不一樣,代表說model產生的image,它的diverse是很大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:48.000" id=12:48.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=768">12:48.000</a></div>
        <div class="t">model產生的image,它是非常diverse的。現在有兩個指標,一個是丟某一張image的時候,它的distribution,另外一個是丟很多張不同的image的時候,它的distribution的平均。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:10.000" id=13:10.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=790">13:10.000</a></div>
        <div class="t">一個是檢查某一張image的quality,另外一個是檢查說產生出來的image有沒有diverse。那這個要怎麼把它結合在一起呢?所以這邊有一個衡量的指標叫做inception score。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:30.000" id=13:30.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=810">13:30.000</a></div>
        <div class="t">這個inception score是proposed在這個improve technique for training gain這篇paper裡面。既然叫inception score,就代表說它那個CNN用的就是inception net。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:44.000" id=13:44.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=824">13:44.000</a></div>
        <div class="t">它的式子就寫這樣,我就直接從paper裡面剪出來,它就說,它其實沒什麼解釋,它就說,就寫這樣,ok,好嗎?那我來解釋一下這個到底是什麼意思。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:04.000" id=14:04.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=844">14:04.000</a></div>
        <div class="t">是這樣子的,這邊有取exponential,但我們就只看exponential裡面的值就好了,因為你知道這個exponential不會影響它的ranking嘛,所以我們看exponential裡面的值就好了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:18.000" id=14:18.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=858">14:18.000</a></div>
        <div class="t">好,我們看exponential裡面的值,它這邊是對x取期望值,對每一個x它再去算p of y given x跟p of y的k of divergence。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:37.000" id=14:37.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=877">14:37.000</a></div>
        <div class="t">如果你要把它列成式子,它應該長什麼樣子呢?如果把它列成式子,應該是長這樣子。雖然我們說是對x取期望值,但是在實作上,你實際做的事情,所謂的對x取期望值呢,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:03.000" id=15:03.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=903">15:03.000</a></div>
        <div class="t">其實就是summation over所有的x。然後呢,這個k of divergence怎麼算呢?這個k of divergence現在的變數是y,所以這邊summation over y,然後算p of y given x,乘上log p of y given x,除掉p of y。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:26.000" id=15:26.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=926">15:26.000</a></div>
        <div class="t">好,那接下來呢,你就可以把log裡面相除的這兩項展開,所以相除變成相減,所以這個式子就可以寫成這兩個式子相減。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:42.000" id=15:42.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=942">15:42.000</a></div>
        <div class="t">那上面這個式子是什麼呢?上面這個式子是p of y given x的entropy取一個負號,對不對?上面這一項是p of y given x這個variable的entropy取一個負號。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:58.000" id=15:58.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=958">15:58.000</a></div>
        <div class="t">那我們會希望,p of y given x,給某一個x的時候,它產生的y的entropy呢,越小越好,所以我們當然希望negative的這個entropy呢,越大越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:14.000" id=16:14.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=974">16:14.000</a></div>
        <div class="t">那下面這一項呢,下面這一整項啊,連同它的負號是cross entropy。那cross entropy大家記得嗎?我們在算這個to classify的時候,我們都是計算你的model的output和正確答案的cross entropy。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:33.000" id=16:33.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=993">16:33.000</a></div>
        <div class="t">所以cross entropy簡單來講就是計算兩個distribution之間的相似度。那我們當然希望說,那我們現在是計算哪兩個distribution之間的相似度呢?我們是計算p of y given x和p of y之間的相似度。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:49.000" id=16:49.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1009">16:49.000</a></div>
        <div class="t">那我們會希望這個相似度越大越好,還是越小越好呢?我們會希望這個相似度是越小越好。因為這兩個相似度不一樣,會代表說你現在產生的這個y跟平均的y很像,代表說你現在產生的y呢,它是非常分散的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:12.000" id=17:12.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1032">17:12.000</a></div>
        <div class="t">現在我給不同的x產生的y呢,是非常分散的。所以你會希望這一項呢,越大越好。所以整體說來呢,我們就會希望整個inception score呢,越大越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:25.000" id=17:25.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1045">17:25.000</a></div>
        <div class="t">這邊是文獻上的一些結果。像在improve wget,我們上次有講那個wget,那個improve版本是算那個gradient的penalty。在這個improve wget裡面呢,它就是用這個inception score來計算它的model的好壞。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:44.000" id=17:44.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1064">17:44.000</a></div>
        <div class="t">好,那在這個improve wget裡面呢,它看起來呢,像是這個樣子。紅色的線呢,是dcget的這個inception score,那橫軸呢,是訓練的iteration。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:01.000" id=18:01.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1081">18:01.000</a></div>
        <div class="t">那黃色和藍色分別是wget,它的這個inception score。那一個是用rnsquad,一個是用addon。那藍色這條線呢,是舊的那個wget,用weight clipping的方法做的,它是藍色這條線。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:21.000" id=18:21.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1101">18:21.000</a></div>
        <div class="t">那這邊想要顯示的就是說,gradient penalty呢,是比weight clipping還要好。然後addon其實略勝於rnsquad。但是這邊你會發現說dcget的performance其實才是最好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:33.000" id=18:33.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1113">18:33.000</a></div>
        <div class="t">比如說wget產生的image,如果從inception score來看的話,假設你相信inception score是對的話,wget呢,並沒有比dcget真的得到比較好的image。只是wget呢,它比較robust。就你把架構弄壞的時候,dcget會壞掉,那wget呢,不會跟著壞掉。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:53.000" id=18:53.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1133">18:53.000</a></div>
        <div class="t">好,那最後要講,然後再來要講的是,有時候你會想要檢查說,你今天你的machine有沒有真的創造出新的image。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:05.000" id=19:05.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1145">19:05.000</a></div>
        <div class="t">因為你希望你的generator它產生的image是database裡面沒有的,machine不是背了database的某一張image,直接把那一張image丟出來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:13.000" id=19:13.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1153">19:13.000</a></div>
        <div class="t">那這邊這張圖要講的事情是,就算是要檢查這件事情,也不見得是容易的。舉例來說,假設你的machine呢,generate了一張圖。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:31.000" id=19:31.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1171">19:31.000</a></div>
        <div class="t">在這個database有一張圖是我現在紅點畫的這一張圖。machine說,我把這張圖往左移一個pixel。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:49.000" id=19:49.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1189">19:49.000</a></div>
        <div class="t">那machine就說,我generate了這張圖。那你要知道說machine是不是直接從database裡面拿一張圖出來的話,你就去計算說,那你的database裡面哪一張圖跟這張圖的image,你就可以看成一個vector。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:03.000" id=20:03.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1203">20:03.000</a></div>
        <div class="t">你用pixel來表述,它就可以看成一個vector。你就看說,哪一個image跟現在machinegenerate的image最像,那machine就會抓到說,它其實只是把這張圖偷偷移一個pixel而已。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:14.000" id=20:14.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1214">20:14.000</a></div>
        <div class="t">那如果machine更聰明一點,它移兩個pixel的話,你就抓不到它了。如果今天它只是把這張圖移兩個pixel,這個時候最近的圖就會變成紅色框框的這一張圖。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:27.000" id=20:27.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1227">20:27.000</a></div>
        <div class="t">那你就會覺得說,這張圖好像是machine真的自己產生的。因為你的database很大,你的database往往有十萬張、百萬張以上的image,你不可能一張一張image去check。所以你看最像的image是這個紅色這一張,你就覺得說,machine的database裡面應該是沒這個看起來像是羊的圖。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:44.000" id=20:44.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1244">20:44.000</a></div>
        <div class="t">你再移幾步,就看起來又是這一張圖。你移三個pixel,最像的是這一張。你移四個pixel,看起來最像的是這一張。那這邊是同樣的例子。把黑色框框這一張圖移一個pixel,最像的還是黑色框框這一張圖。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:59.000" id=20:59.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1259">20:59.000</a></div>
        <div class="t">移兩個pixel,就變成這一張。移三個pixel,就變成這一張。移四個pixel,就變成這一張。所以machine捐了一台卡車,你也不一定要太高興,因為它搞不好只是把database的卡車的pixel挪動而已。但是你檢查不出來,因為你會以為說database裡面完全沒有卡車,最像的可能就是這個飛機。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:17.000" id=21:17.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1277">21:17.000</a></div>
        <div class="t">然後我們都知道會有mock collapse的問題,那這邊放了一個在celeb A,就是明星的corpus上面的mock collapse的結果。這個是我在,我剛才講的不是有一個game的talk嗎?那是在那個talk裡面的圖。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:38.000" id=21:38.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1298">21:38.000</a></div>
        <div class="t">如果你今天trade到最後,你在那個celeb A的corpus上trade到最後,你得到的image看起來是這個樣子的。這些image都有一個共通性,比如說非常的蒼白,然後他們都露出一個奇怪的微笑,牙齒都很白,看起來就像吸血鬼一樣,就長這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:59.000" id=21:59.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1319">21:59.000</a></div>
        <div class="t">那在這種case你可以很容易地看出這個是mock collapse,但有時候是missing mode,就是有某些mode不見,但是你不會知道有哪些mode不見,因為你沒有辦法看過整個database啊,所以你不知道說有哪些case是machine從來沒有畫過那種。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:17.000" id=22:17.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1337">22:17.000</a></div>
        <div class="t">舉例來說,這邊有一對machine generate的人臉,你可以看出來machine掃generate了什麼嗎?你可以看出來machine掃generate了什麼嗎?這邊男的女的都有,然後也有有戴眼鏡也有沒戴眼鏡。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:33.000" id=22:33.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1353">22:33.000</a></div>
        <div class="t">那machine到底掃generate什麼呢?在右上角引用的這篇paper裡面,他們發現說,比如說在這個case裡面,machine沒有generate什麼的,他沒有generate有戴帽子的人,對不對?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:49.000" id=22:49.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1369">22:49.000</a></div>
        <div class="t">這個人是小智,他戴這個帽子,他小智。這邊他沒有generate沒有戴,他generate的人都是沒有戴帽子的,但是你不會知道這件事,因為你沒有辦法看過所有的collapse,你根本不知道說collapse裡面有一些人是有戴帽子的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#23:07.000" id=23:07.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1387">23:07.000</a></div>
        <div class="t">那他怎麼知道這件事呢?他的做法其實是說,他train了一個discriminator,然後把database裡面的image當作positive sample,然後把machine generate image當作next example,然後去predict一下database裡面的每一個image。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#23:23.000" id=23:23.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=VNqOspvEKEI&t=1403">23:23.000</a></div>
        <div class="t">然後發現說,有一些image他被當作positive sample的case是特別的confidence是特別特別特別高的,有一些case是特別特別高的,那比如說像這邊這張image被discriminator認為是real的confidence很高,那他可能就是在一個missing mode裡面,在generate的data裡面,不會generate類似的圖片。</div>
    </div>
    
</body>
</html>   