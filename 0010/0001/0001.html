<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Hung-yi Lee's Lectures(台大李宏毅老師系列課程)</title>
    <style>
        body {
            font-family: sans-serif;
            font-size: 18px;
            color: #111;
            padding: 0 0 1em 0;
        }
        .l {
          color: #050;
        }
        .s {
            display: inline-block;
        }
        .e {
            display: inline-block;
        }
        .t {
            display: inline-block;
        }
    </style>
  </head>
  <body>
    <a href="../../index.html">back to index</a>
    <h2>Deep Learning Theory 1-2: Potential of Deep</h2><a href=https://www.youtube.com/watch?v=FN8jclCrqY0><img src=https://i.ytimg.com/vi_webp/FN8jclCrqY0/hqdefault.webp></a><br>
    
    <div class="c">
        <a class="l" href="#00:00.000" id=00:00.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=0">00:00.000</a></div>
        <div class="t">為什麼我們要用deep呢?接下來就是要講我們要用deep的理由。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:30.320" id=00:30.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=30">00:30.320</a></div>
        <div class="t">但這跟三年前講的東西是不一樣的。三年前講的時候,我們只有直覺而已。但是現在在三年後,deep learning發展得很快,不止有了直覺,還有了更多的理論的基礎,所以跟之前講的東西是不一樣的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:45.740" id=00:45.740>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=45">00:45.740</a></div>
        <div class="t">那這些deep比較好這件事情,在很早以前就有這樣子的猜想。舉例來說,你看那個楊瓦克納他們在09年的時候寫了說為什麼我們要用deep network的時候,裡面就有各式各樣的猜想告訴我們說deep是比較好的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:04.640" id=01:04.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=64">01:04.640</a></div>
        <div class="t">只是在過去並沒有太多的理論的證明,但是現在已經有了很多理論的證明了,然後把相關理論證明的paper都附在這個投影片的後面,你有興趣的話再參考。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:16.000" id=01:16.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=76">01:16.000</a></div>
        <div class="t">今天所講的內容並不是base上某一篇paper的某一個理論講的,因為你知道上課嘛,上課要講的東西還是希望你可以聽得懂這樣子,所以是稍微做了一些簡化,希望大家是可以聽得懂的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:29.680" id=01:29.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=89">01:29.680</a></div>
        <div class="t">好,那為什麼我們要用deep呢?那我們都知道說,雖然shallow的network可以表示任何的function,你可以用shallow的network去fit任何的L-listed function到任何的精準度,只要neuron夠多,但是沒有告訴你的是,今天所謂的neuron夠多到底要多到什麼樣的地步。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:52.560" id=01:52.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=112">01:52.560</a></div>
        <div class="t">那我們剛才已經講說,其實你neuron的數目是L除以x0嘛,是,neuron的數目是L除以x0,接下來要看的就是說,如果我們用deep呢,好,加一個你說的對,是兩倍,是兩倍。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:08.700" id=02:08.700>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=128">02:08.700</a></div>
        <div class="t">好,那為什麼我剛才沒有加兩倍呢?是因為其實你永遠可以想得到更好的方法來fit這個東西,就是說,我剛才是用比較笨的方法,用兩個neuron才製造一條直線,那其實你仔細想一想,應該用一個neuron就可以製造一條直線了,所以比較好的寫法也許就是放一個big O這樣子,big O的L除以x0,意思就是說前面,不知道big O是什麼呢?就是說,這前面有一個常數嘛,我們才確定那個常數是什麼這樣,好,謝謝。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:39.060" id=02:39.060>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=159">02:39.060</a></div>
        <div class="t">好,那我們等一下就要講說,deep可不可以做得比這個好,還有可不可以做得比這個好,好,那,但是呢,在講理論的部分之前,我們還先講回顧一下在過去十年來人們是怎麼樣,那過去的時候通常就是,不是理論的證明,就是舉個例子嘛,就舉個例子給你聽,打個比方,就打個比方說,為什麼我們需要deep,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:03.180" id=03:03.180>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=183">03:03.180</a></div>
        <div class="t">舉例來說,你在寫程式的時候,其實你知道任何的演算法都可以用兩行程式就implement嗎?就好像說打大象塞進冰箱只要三個步驟一樣,其實任何演算法都可以用兩行程式就implement,怎麼implement呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:19.380" id=03:19.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=199">03:19.380</a></div>
        <div class="t">舉例來說,假設今天的演算法是sorting的演算法,你就把所有sort前的字串通通找出來,窮取所有可能的input,把每一個output的可能通通事先算好,當作它的key,當作它的value,所有可能的input都是key,所有可對應的output就是value,所以A是一個數字的sequence,A'是它sorting好的結果,假設你要implementsorting的演算法的話,存一個巨大的table把它存好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:48.560" id=03:48.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=228">03:48.560</a></div>
        <div class="t">接下來程式就這樣寫,input一個sequence叫做K,然後第一行就是查表,call一個function叫做matchkey,看看這邊有沒有那個key出現,然後把那個key落在第幾個row的row的id回傳回來,然後再看那個row的id對到的value是什麼,把那個value吐出去,你就implement完任何演算法了,結束。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:12.560" id=04:12.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=252">04:12.560</a></div>
        <div class="t">所以我們知道說,其實任何演算法都可以用兩行程式來implement,但是沒有人實際,沒有人用這樣子的方法來implementsorting,那這個其實像這樣子的方法,你仔細想想看,它有點像是SVM加上kernel對不對,如果大家熟悉SVM的話,SVM加上kernel是怎麼implement的,就是你有一筆dataX進來,然後這邊的n代表說所有的training data,你training data裡面還有沒有n筆,X1到X2,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:41.020" id=04:41.020>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=281">04:41.020</a></div>
        <div class="t">而你把每一筆data,你input的dataX跟每一筆training dataXn,都去計算它們的相似度,k log Xn跟X代表它們之間的相似度,那只是不同的kernel代表你使用了不同的相似度,好,X跟Xn去計算了相似度以後,然後再乘上αn,最後就得到它的offer。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:02.560" id=05:02.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=302">05:02.560</a></div>
        <div class="t">所以這個計算相似度這件事啊,其實就是key matching,乘上αn再輸出,其實就是你得到row id以後,把你的結果輸出來,只是如果SVM with kernel的話,它不是只抽一個key而已,它會算跟不同的key有不同的match程度,然後把你的value做weighted上,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:23.180" id=05:23.180>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=323">05:23.180</a></div>
        <div class="t">但是其實這個SVM加上kernel在本質上,它在解這個machine learning的問題的時候,它在描述一個function的時候,其實就很像是這種兩行的演算法,但是我們知道說,你不會用兩行code來implement algorithm,你會用好多個step來implement algorithm,為什麼?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:42.660" id=05:42.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=342">05:42.660</a></div>
        <div class="t">因為這樣是比較有效率的方式,你不需要存一個碩大無朋的table,這個是一個比喻,還有另外一個比喻是用circuit來做比喻,假設你是電機系的同學的話,你一定修過邏輯電路設計,這個是必修。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:02.800" id=06:02.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=362">06:02.800</a></div>
        <div class="t">那在邏輯電路裡面,我們學到說,所有的邏輯電路都是由gate所構成的,就好像neural,neural network都是由neural所構成的,那我們知道說,只要兩層的邏輯閘就可以組成任何的boolean function,我們都知道說,只要一個hidden layer的neural,一個hidden layer的neural其實也是兩層,hidden layer跟offer layer也是兩層,它可以表示任何的continuous function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:28.480" id=06:28.480>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=388">06:28.480</a></div>
        <div class="t">但是你不會用兩個層的邏輯閘來設計電路,因為設計出來的電路太過龐大,如果你有用hierarchical的結構的話,你有用motor layer的結構的話,設計出來的電路會是比較精簡的,如果你用deep的結構來設計電路,你的電路裡面的邏輯閘是有好幾層的,那你設計出來的電路會比較精簡,你要做同樣的事情,只需要比較少的gate。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:53.180" id=06:53.180>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=413">06:53.180</a></div>
        <div class="t">對neural來說,過去也不怎麼證明,就打個比方,比如說A是這個樣子,然後B應該也是一樣吧,如果有很多個layer的neural,你要描述一個function的時候應該比較容易,所以你只要用deep的結構,需要比較少的neural,就可以描述同樣的function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:15.440" id=07:15.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=435">07:15.440</a></div>
        <div class="t">那這樣可能會被反駁說,就像A很像B,並不代表他們性質就是一樣的,但他們確實是非常像的,所以理論可能是相通的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:26.020" id=07:26.020>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=446">07:26.020</a></div>
        <div class="t">好,那下一頁這個例子,我打算把它略過,你可以自己看一下,當我們今天講到network的架構,把network的架構跟deep的network把它勾在一起的時候,通常就舉這個例子,就告訴你說,在電路裡面,如果你要implement一個叫做parity check的電路,你可以用shallow的network來implement,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:51.580" id=07:51.580>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=471">07:51.580</a></div>
        <div class="t">但是如果你input的sequence長度有D的話,你要big O2的D次方的gate,但是假設你今天用一個deep的架構來implement,而實際上的細節就不管了,反正就弄成這個樣子,一個deep的架構,那你其實只需要big O2的gate就可以做到這件事了,我想這個是大家都知道的事情,但這就只是打個比方而不是一個證明。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:17.940" id=08:17.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=497">08:17.940</a></div>
        <div class="t">好,那接下來我們要講說,deep為什麼它有潛能,有機會可以比這個shallow的network更好呢?在很早,我所謂早年是大概14年、15年那個時候,這個東西是那個Ian Garfellow教科書裡面都有寫的,所以它比較早的東西,我們就知道說,假設一個network它是shallow跟wide,跟假設另外一個network它是deep而narrow的,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:46.660" id=08:46.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=526">08:46.660</a></div>
        <div class="t">在它們有差不多參數量的情況下,這個時候,shallow和wide的network,它可以產生出來的piecewise linear function的piece是比較少的,而deep and narrow的架構,它可以產生出來的piece是比較多的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:05.620" id=09:05.620>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=545">09:05.620</a></div>
        <div class="t">如果你今天有一個shallow的network,它可以產生出來,在shallow跟deep的network在同樣的參數量的情況下,shallow的network可以產生出來的piece是比較少的,deep的network可以產生出來的piece是比較多的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:21.620" id=09:21.620>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=561">09:21.620</a></div>
        <div class="t">怎麼說呢?我們先來看看,假設給你一個network的架構,隨便拿一個relue的network給你,這個network會有多少的piece呢?如果它是piecewise linear function,它會有多少個linear的片段呢?它會有多少個piece呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:43.940" id=09:43.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=583">09:43.940</a></div>
        <div class="t">我們先來看一下它的upper bound,就是最多會有多少,怎麼想它的upper bound呢?我們大家都對relue的network都很熟嘛,我們知道在relue的network裡面,每一個neuron有兩個operation的region,對不對,一個operation的region是這個neuron的output是0,另外一個operation的region是input等於output。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:09.380" id=10:09.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=609">10:09.380</a></div>
        <div class="t">我們會到之前在machine learning的課裡面,我們都講過relue的network,然後它是會告訴你說,0的那些就把它拿掉,因為它好像不存在一樣,剩下的部分就好像是一個linear的function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:23.220" id=10:23.220>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=623">10:23.220</a></div>
        <div class="t">這時候很多人就問我說,所以relue是linear的function,那不是很弱嗎?但它不是完全是linear function,它是一個piecewise linear function,當今天你的neuron都用在同樣的region的時候,它是一個linear function,但是它今天換了,有些neuron的region,它的operation的狀態換了的時候,它就進入了另外一個linear的region。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:51.760" id=10:51.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=651">10:51.760</a></div>
        <div class="t">這樣大家應該知道我的意思吧,就是假設這個是你的input x,這個是你的output y,在某種activation function的operation的mode下面,它是一個linear function,但是一旦今天input超過某個範圍,某一個neuron的operation那個mode換掉了,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:16.460" id=11:16.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=676">11:16.460</a></div>
        <div class="t">就本來它可能是output都是0,現在變成input x output,不然它本來input x output現在變成output都是0,那你就變成另外一個linear的function了,所以今天如果我們來好分析一個relue的network,它最終有幾個piece,它的upper bound應該就是看你有幾個neuron,每一個neuron它都可以是作用在兩個mode的其中一個,所以所有的neuron跟mode的可能性,我們就這邊叫它activation的pattern。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:46.160" id=11:46.160>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=706">11:46.160</a></div>
        <div class="t">所以activation pattern的意思就是某一種neuron的mode的組合,比如說這邊是linear linear linear,這個叫做一種pattern,那你也可能有別種pattern,總共有多少種pattern呢,這個是國小數學,每一個neuron有兩種可能性,所以假設n個neuron就是2的n次方的可能的activation pattern,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:09.660" id=12:09.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=729">12:09.660</a></div>
        <div class="t">所以有n個neuron,2的n次方的activation pattern,8個neuron會有2的8次方的可能的activation pattern,每一個activation的pattern都製造了一個linear的function,所以想起來好像是有n個neuron,有2的n次方的activation pattern,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:28.080" id=12:28.080>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=748">12:28.080</a></div>
        <div class="t">那我們用這一個network架構所定出來的function應該要有2的n次方的linear的pieces,有2的2次方的片段,但這個其實是一個upper bound,這是一個最佳的狀況,你不可能,事實上,為什麼這是一個最佳的狀況呢,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:51.200" id=12:51.200>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=771">12:51.200</a></div>
        <div class="t">因為為什麼這個狀況不一定能夠達到呢,因為有些pattern可能是永遠沒有辦法出現的,有些pattern是不可能出現的,舉例來說,我們舉一個這樣子的例子,這是一個非常簡單的review的network,只有一層,只有兩個neuron,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:06.980" id=13:06.980>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=786">13:06.980</a></div>
        <div class="t">我們剛才說,每一個neuron有2個mode,所以今天按照這個network,它的activation pattern其實應該要有4種,但是你實際上去想一想,你會發現說,這一個network的架構,它只定義得出3個pieces而已,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:33.260" id=13:33.260>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=813">13:33.260</a></div>
        <div class="t">它定義不出4個pieces,對不對,因為有一些operation的狀態是不合理的,是沒有辦法出現的,我不知道大家能不能夠接受這個想法,你可以回去想一下,你把這一個network的架構,你把它的參數是各種不同的參數,你挪來挪去,你可能都只製造得出3個pieces而已,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:59.260" id=13:59.260>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=839">13:59.260</a></div>
        <div class="t">因為你只能產生3種activation pattern,你沒有辦法產生4種activation pattern,那其實事實上呢,今天理論上,假設我們有一個network,它是只有一個hidden layer,有n個neuron,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:18.160" id=14:18.160>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=858">14:18.160</a></div>
        <div class="t">本來按照阿噴棒來想,應該要有2個n次方的activation pattern,它應該可以產生2個n次方的pieces,但是如果你仔細想一下的話,你會發現說,今天只有一個hidden layer的network,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:35.540" id=14:35.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=875">14:35.540</a></div>
        <div class="t">它假設那個hidden layer的neuron是n,其實它弄得出來的pieces的數目,其實只是big O的n而已,可能是n加1這樣,你沒有辦法弄出2個n次方的不同的pieces。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:48.540" id=14:48.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=888">14:48.540</a></div>
        <div class="t">好,那這邊我們剛才講的是阿噴棒,所以從阿噴棒看起來,雖然我們知道說那個阿噴棒實際上可能是達不到的,就是說我們說今天給我們一個relu的network,它最多可以製造出來的function會有幾個linear的pieces呢,最多是2的n次方的pieces,但這是一個阿噴棒,很有可能怎麼調都達不到那一個pieces的數目。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:16.540" id=15:16.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=916">15:16.540</a></div>
        <div class="t">接下來我們要講的是,那lower bound呢?你要講lower bound很簡單,就是兜一個network,看看我們可以製造出多少的pieces,然後那個pieces就是我們可以製造出來的pieces的數目的lower bound。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:32.540" id=15:32.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=932">15:32.540</a></div>
        <div class="t">所以我們就是講一個network,找一個relu的network,然後告訴你,然後分析一下說,我們根據這個relu的network,我們可以製造出多少的pieces。好,那在講這一段之前呢,我們先製造一個特別的activation function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:51.540" id=15:51.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=951">15:51.540</a></div>
        <div class="t">這個特別的activation function叫做取絕對值的activation function,也就是說你input是x,我們把x乘上w再加上bias b,通過這個activation function以後呢,會取絕對值。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:05.540" id=16:05.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=965">16:05.540</a></div>
        <div class="t">那怎麼implement這種取絕對值的activation function呢?其實你可以把兩個relu的activation function組合起來,就可以變成一個取絕對值的activation function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:21.540" id=16:21.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=981">16:21.540</a></div>
        <div class="t">我們有兩個relu的neuron,今天x進來,走上面這個relu的neuron的時候,它乘以w加上bias b,走下面這個relu的neuron的時候,它乘上-w減掉bias b,然後今天呢,這兩個都是relu的activation function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:41.540" id=16:41.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1001">16:41.540</a></div>
        <div class="t">所以如果今天wx加b大於0的話,那就是拿這邊的輸出,這邊的輸出就是0嘛。那如果wx加b小於0的話,就是拿這邊的輸出,這邊就是0嘛。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:57.540" id=16:57.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1017">16:57.540</a></div>
        <div class="t">所以仔細想一下就會知道說,用這個方法,你用兩個relu的activation function,你可以製造一個取絕對值的activation function。你可以去製造一個像這樣子取絕對值的activation function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:14.540" id=17:14.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1034">17:14.540</a></div>
        <div class="t">好,那接下來呢,我們要講的事情是,如果我們現在有這樣子的activation function的話,我們就放一個這樣子的relu,然後我們的input是x,我們的output是a1。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:36.540" id=17:36.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1056">17:36.540</a></div>
        <div class="t">那我們input的x跟output的a1間,可能會有什麼樣的關係呢?他們的關係可能是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:48.540" id=17:48.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1068">17:48.540</a></div>
        <div class="t">在0到1二分之一中間,它的值呢,是從1逐漸下降到0。從1二分之一到1中間,今天output a1的值是從1二分之一逐漸上升到1這樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:06.540" id=18:06.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1086">18:06.540</a></div>
        <div class="t">好,那我們假設說,第一個hidden layer做的事情就是這個樣子。好,那我們現在假設我們加上第二個hidden layer,第二個hidden layer就是把a1吃進去,然後變成a2吐出來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:20.540" id=18:20.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1100">18:20.540</a></div>
        <div class="t">我們假設第二個hidden layer跟第一個hidden layer做的事情,其實是一樣的。a1跟a2的關係和x跟a1的關係其實是一樣的。當a1的變化從0到1二分之一的時候,a2從1下降到底。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:37.540" id=18:37.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1117">18:37.540</a></div>
        <div class="t">當a1從1二分之一變到1的時候,a2從0上升到底。x1跟a1的關係跟a1和a2的關係其實是一樣的。好,那我們知道x1跟a1的關係,我們知道a1跟a2的關係,那這整個function長什麼樣子呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:53.540" id=18:53.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1133">18:53.540</a></div>
        <div class="t">也就是說這個x跟a2之間的關係看起來像是什麼樣子呢?我們來想一下。input是從0到1,我們現在就是要看說x從0到1的時候,這個a2是怎麼樣變化。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:12.540" id=19:12.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1152">19:12.540</a></div>
        <div class="t">那今天這個a2呢,在二分之一的地方,在a1是二分之一的地方,會有一個轉折的點。所以我們在考慮的時候呢,畫一個a1等於二分之一的線,然後呢,從0到1之間呢,我們分成四個區段來考慮。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:29.540" id=19:29.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1169">19:29.540</a></div>
        <div class="t">我們就考慮0到四分之一,考慮0到四分之一,四分之一到二分之一,二分之一到四分之三,四分之三到一,分成四個片段來考慮。而如果考慮第一個片段,考慮第一個片段,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:46.540" id=19:46.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1186">19:46.540</a></div>
        <div class="t">x從0變到二分之一的時候,x從0變到二分之一的時候,a1從1變到二分之一,a1從1變到二分之一。當a1從1變到二分之一的時候,a2呢,它是從1變到0。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:05.540" id=20:05.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1205">20:05.540</a></div>
        <div class="t">所以今天x從0到四分之一這段距離,當從0到四分之一這段距離有變化的時候呢,a2是從1變到0。這個是第一段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:17.540" id=20:17.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1217">20:17.540</a></div>
        <div class="t">那如果你分析第二段從四分之一到二分之一的時候,x的變化從四分之一到二分之一,a1的變化是從二分之一一直跑到0,它從二分之一一直跑到0。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:33.540" id=20:33.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1233">20:33.540</a></div>
        <div class="t">這個其實非常難想像啊,我不知道大家聽不聽得懂這樣子。如果你聽不懂的話,你就自己回去,自己仔細想一想這個,這個畫圖也不知道要怎麼畫才好這樣子。從這一段,x是從四分之一到二分之一,那a1呢,是從二分之一變到0,所以a1是從二分之一變到0。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:52.540" id=20:52.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1252">20:52.540</a></div>
        <div class="t">a2呢,是從0變到1。所以x從這邊到中間,從四分之一到二分之一的時候,a2是從0變到1的,所以是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:03.540" id=21:03.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1263">21:03.540</a></div>
        <div class="t">好,再講下去你可能就會覺得有點無聊了,所以今天x從二分之一變到四分之一,那a2會有什麼樣的變化呢?它的變化是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:13.540" id=21:13.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1273">21:13.540</a></div>
        <div class="t">x從四分之三變到一,a2的變化是這個樣子。所以它畫了一個w的形狀,它畫了一個w的形狀。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:25.540" id=21:25.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1285">21:25.540</a></div>
        <div class="t">好,所以今天我們知道說,a2的輸出是這個function,有兩個neuron的function,它是這個w的形狀。好,那我們再加上第三個neuron呢?如果再加上第三個neuron的話,會發生什麼樣的事情呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:43.540" id=21:43.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1303">21:43.540</a></div>
        <div class="t">我們假設第三個neuron跟前面兩個neuron做的事情是一模一樣的,只是input是a2,output是a3,那a2跟a3的關係長得是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:55.540" id=21:55.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1315">21:55.540</a></div>
        <div class="t">當我們加上這個紅色的線的時候呢,你就會發現說,你加上這個紅色的線以後,接下來你就分析一下這個x從0變到1。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:05.540" id=22:05.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1325">22:05.540</a></div>
        <div class="t">你要分成八個區間去考慮,你要從0分析到八分之一,就ax從0變到八分之一的時候,a2是從1跑到二分之一,是從1跑到二分之一。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:24.540" id=22:24.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1344">22:24.540</a></div>
        <div class="t">所以a3會從1變到0,所以a3會從1變到0。然後你就把每一個piece,每一個小段,這個x跟a2,然後a2到a3的關係通通畫出來,你看起來就是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:41.540" id=22:41.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1361">22:41.540</a></div>
        <div class="t">所以本來是一個w的形狀,現在變成兩個w的形狀,變成兩個w連在一起,變成一個很多句尺的形狀。所以現在你就會發現說,本來a1它是長這個樣子的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:57.540" id=22:57.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1377">22:57.540</a></div>
        <div class="t">a2跟a3的關係跟x跟a1的關係是一樣的,它總共有兩個線段,也就是二個一次方的線段。到a2的時候,它總共有四個線段,二個二次方的線段。到a3的時候,它就變成有二個三次方的線段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#23:16.540" id=23:16.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1396">23:16.540</a></div>
        <div class="t">所以你會發現說,今天當我們用deep structure的時候,每次我多加了一個layer,那其實我的每一個layer只有兩個neuron,我每次多加了兩個neuron的時候,我們可以產生的linear region,我們可以產生出來的線段的數目就會變成兩倍。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#23:41.540" id=23:41.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1421">23:41.540</a></div>
        <div class="t">就每多加兩個,你的線段的數目就會double。所以如果我們今天比較shallow的network跟deep的network,在講shallow的network的時候,我們每次兩個neuron組合起來才產生一個線段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#23:55.540" id=23:55.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1435">23:55.540</a></div>
        <div class="t">所以如果今天你要產生一百個線段,你就要兩百個neuron。但是今天如果是deep的structure,每次多加了兩個neuron,這邊這個絕對值的這個neuron它是兩個ray路所組成的,每次多加一個layer,那個layer就只有兩個neuron,你都可以讓你的線段的數目增加兩倍。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#24:16.540" id=24:16.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1456">24:16.540</a></div>
        <div class="t">所以你今天舉例來說,你要產生一百個線段,一百個線段是多少?我只要七次方,對不對?我只要七個neuron,我就可以做到那件事情了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#24:34.540" id=24:34.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1474">24:34.540</a></div>
        <div class="t">所以我發現,你要產生有比較多片段的function的時候,用deep是比較有效率的。當然如果你仔細回想一下,為什麼deep可以產生比較多的線段呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#24:53.540" id=24:53.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1493">24:53.540</a></div>
        <div class="t">它做的事情比較像是摺紙一樣,或者它其實是把同樣的pattern反覆地出現。本來你只有一個V,然後第一層只有一個V,第二層就把兩個V接起來變成W,第三層就是把兩個W接起來,就變成有兩個W拼在一起。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#25:14.540" id=25:14.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1514">25:14.540</a></div>
        <div class="t">接下來下一層就是把兩個W再double,變成有四個W。它是把原來你的piece不斷地反覆產生。不知道大家能不能體會,它就像是雪花結晶的結構那樣,它不斷地把原來的pattern不斷地複製。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#25:34.540" id=25:34.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1534">25:34.540</a></div>
        <div class="t">它可以產生很多的piece,但那些piece是有規則的。講了這麼多以後,其實你可以非常輕易地證明說,假設你現在的network寬度是K,深度是H,你可以製造K的H次方的片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#25:57.540" id=25:57.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1557">25:57.540</a></div>
        <div class="t">我們剛才是寬度是2,然後深度是H,我們可以製造2的H次方的片段。現在你可以輕易地自己想出來說,假設現在寬度不是2,而是K,那深度是H,你可以製造出K的H次方的片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#26:21.540" id=26:21.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1581">26:21.540</a></div>
        <div class="t">這個你其實可以非常輕易地想出,找到一個network可以做到一件事情。這個東西就是這個network架構可以產生的piece的數目的lower bound。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#26:32.540" id=26:32.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1592">26:32.540</a></div>
        <div class="t">所以今天這個結果告訴我們說,你可以產生的片段的數目是K的H次方,所以H,也就是深度,是放在指數的地方。所以當你增加你的深度的時候,你可以非常快地增加piece的數目。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#26:51.540" id=26:51.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1611">26:51.540</a></div>
        <div class="t">而如果你想要知道更多更細的證明的話,下面列了一打paper。舉例來說,前面兩篇是Benjo的paper,其實是最早做這樣子分析的paper。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#27:02.540" id=27:02.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1622">27:02.540</a></div>
        <div class="t">因為我們今天在上課的時候,我們都假設input只有一個dimension嘛,他們不是這樣,他們會假設比較複雜的case,input是第一個dimension,這個時候狀況就比較複雜了,就沒有今天得到的式子那麼單純。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#27:18.540" id=27:18.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1638">27:18.540</a></div>
        <div class="t">原來在ICL2014的paper裡面,他們得到了一個lower bound,後來他們又在下一篇NIST2014裡面又improve了那個lower bound,後來也有很多人做類似的嘗試去繼續improve那個lower bound,那就把這個文獻列在這邊給大家參考。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#27:36.540" id=27:36.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1656">27:36.540</a></div>
        <div class="t">我想要講的是最後一篇,在最後一篇裡面除了有理論的證明以外,他還做了一些實驗,因為我們剛才講的只是一個理論上是這個樣子,我們用了一個很奇怪的方法,兜出了一個relu的network,然後分析說,嗯,這個relu的network有很多很多的piece。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#27:55.040" id=27:55.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1675">27:55.040</a></div>
        <div class="t">可是你就覺得說,欸,這個會不會是一個非常非常specific的case,也許在一個正常的狀況下,你train一個network根本就不會產生那麼多的piece。所以他就做了一些實驗來verify這件事,而他一個實驗是做在NIST上面。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#28:13.040" id=28:13.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1693">28:13.040</a></div>
        <div class="t">在NIST上面,你這個有不同的,我先看第一個圖,第一個圖的橫坐標代表的是network的深度,就兩層四層六層八層十層十二層等等,那不同的顏色代表不同的寬度,五十個neuron,一百個neuron,五百個neuron,七百個neuron等等。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#28:31.540" id=28:31.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1711">28:31.540</a></div>
        <div class="t">而這個SCL啊,大家不用太在意它,它是不同的,train network的時候不同的initialization的參數。那接下來他這邊做的實驗就是,他把這個network先拿出來,然後再看說input從某一點到某一點的時候,output總共通過了多少個piece。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#28:49.940" id=28:49.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1729">28:49.940</a></div>
        <div class="t">了解嗎?就是network就是一個function,就是一個piecewise linear function,然後他就算說從某一點到某一點總共經過了多少個pieces。好,那今天這個圖啊,縱軸就是pieces的數目。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#29:04.340" id=29:04.340>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1744">29:04.340</a></div>
        <div class="t">那注意一下這個縱軸,它是那個exponential的哦,它是exponential的。所以今天這個直線的上升,其實是exponential的上升。所以今天固定你的network的size,固定network的寬度,不是size,固定network的寬度,只增加深度的時候,你會發現這個時候你產生的pieces的量,就算在實際的case,它也是exponential增加的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#29:29.840" id=29:29.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1769">29:29.840</a></div>
        <div class="t">我們剛才講說那個阿寬棒不是寬k的那個深度h次方嗎?那在,但這個是一個理論上的想像,但是在實際上,在實際的application上,也可以觀察到這樣的現象。當你的depth增加的時候,你產生的pieces的數目是exponential增加的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#29:49.500" id=29:49.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1789">29:49.500</a></div>
        <div class="t">另外這個實驗是layer的數目固定,但是橫軸是改變layer的寬度,有200個neuron,400個neuron,600個neuron,800個neuron等等,縱軸一樣是exponential。不同的顏色代表不同的深度,2層,4層,6層等等。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#30:06.140" id=30:06.140>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1806">30:06.140</a></div>
        <div class="t">你會發現說如果今天是固定你的深度,但是只改變你的寬度的時候,對產生的pieces的數目影響就不大,這個線基本上看起來像是直線一樣。這是第一個實驗,他還做了另外一個有趣的實驗是說,他講了他這樣做,就拿出一個network,他paper裡面其實沒有講得很清楚那個network是哪來的,拿出一個network有很多層。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#30:31.900" id=30:31.900>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1831">30:31.900</a></div>
        <div class="t">他在input的space上面畫一個圈圈,假設input是二維的畫一個圈圈,通過第一個hidden layer以後,那些neuron不是會有output嗎?不是會變成一個100維的vector嗎?input二維,假設你的第一個hidden layer是100維,它會變成一個100維的點。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#30:49.760" id=30:49.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1849">30:49.760</a></div>
        <div class="t">但是你在input的時候你是畫一個圈圈,那在高維的空間中你也是走了某一個軌跡,假設你的hidden layer size是100維,那在100維的空間中你也是走了某一個軌跡,只是它不見得是圓的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#31:05.760" id=31:05.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1865">31:05.760</a></div>
        <div class="t">他就把100維的空間中的軌跡軌跡到二維,他說看起來像是這個樣子,這是第一個layer。第二個layer看起來像是這個樣子,第三個layer看起來像是這個樣子,第四個layer看起來就是這個樣子,就越來越複雜。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#31:18.660" id=31:18.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1878">31:18.660</a></div>
        <div class="t">本來你在input的時候你只是畫了一個圈圈,但是通過很多個layer以後,這個軌跡在network的output看起來越來越複雜,直到最後變得非常複雜。但是發現說,這個複雜的結構裡面,其實你仔細看一下,它是有pattern的,它並不是一個完全隨機的複雜的結構,而是有某一些的對稱性。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#31:42.860" id=31:42.860>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1902">31:42.860</a></div>
        <div class="t">這邊有某一些有趣的對稱性,就好像是雪花那個樣子。所以就呼應我們剛才講的,說network可以產生很多的piece,那產生的這些piece中間,它是有某一種pattern的,它是把v變成w,再把兩個w接起來,它是有一個固定的pattern,它不是隨機的產生那些piece。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#32:04.760" id=32:04.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1924">32:04.760</a></div>
        <div class="t">那個pattern還有另外一個實驗,這個實驗想要驗證的是說,low layer的參數,就比較靠近input的那個layer的參數,相較於比較靠近output layer的參數,它是比較重要的,就比較靠近input的那些參數是比較重要的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#32:21.760" id=32:21.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1941">32:21.760</a></div>
        <div class="t">因為在直覺上想起來這樣也是有道理的,因為我們今天在做deep learning的時候,就好像是在摺紙一樣,前面的那些layer做的事情就是去摺紙。那在摺紙的時候,第一次對摺的時候是最重要的,對不對?你第一次對摺就歪掉了,下面你再摺,通通都是歪的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#32:38.920" id=32:38.920>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1958">32:38.920</a></div>
        <div class="t">所以這個實驗想要驗證的是,low layer的參數是比較重要的,那怎麼驗證呢?它先做了一下Cybertek,它在Cybertek上拿出一個正確率非常高的network,然後在network的參數上面加一些noise,分別加在第一層、第二層,一直加到第七層。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#32:59.560" id=32:59.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1979">32:59.560</a></div>
        <div class="t">那發現說假設現在那些noise是加在最後第七層的話,對結果幾乎沒有影響,下面這個是noise越加越大。對於縱軸這個正確率,本來正確率是100%,但是你加一些noise幾乎沒什麼影響。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#33:16.140" id=33:16.140>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=1996">33:16.140</a></div>
        <div class="t">但是如果你加在第一層,一樣的noise加在第一層,整個結果就壞掉了,整個結果就突然爆炸,顯示說第一層的network是非常sensitive的,你只要稍微加一點noise它就會壞掉了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#33:29.400" id=33:29.400>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2009">33:29.400</a></div>
        <div class="t">另外這個實驗如果沒記錯應該是做在at least的上面,縱軸式正確率,這個實驗是這樣子的,這個實驗是說拿一個network出來,我們只確某一個layer,其他layer的fixed數都是random的,就只確一個layer,其他都是fixed數,然後這邊的顏色跟這邊是一樣的,所以這個紫色就是這邊的紫色。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#33:50.820" id=33:50.820>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2030">33:50.820</a></div>
        <div class="t">假設我們只確第一層,第二層到最後一層通通是random的話,其實你也可以得到大概90%的正確率。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#34:00.000" id=34:00.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2040">34:00.000</a></div>
        <div class="t">其實這沒有很尷尬,at least胡亂做都是98%,所以90%是很差,但是神奇的就是說,我只認了第一層,後面都是random,還是有90%,顯示第一層其實非常的重要。但是假設我們是說前面都是random,只認最後一層,其實結果就很爛,所以這告訴我們說deep network裡面,前面的layer是比較sensitive、比較重要的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#34:27.680" id=34:27.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2067">34:27.680</a></div>
        <div class="t">好,我們在這邊休息十分鐘,等下再回來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#34:31.900" id=34:31.900>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2071">34:31.900</a></div>
        <div class="t">現在要講的是用shallow的network來fit某一個function,然後剛才又講了說,假設比較deep和shallow的network的話,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:00.640" id=35:00.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2100">35:00.640</a></div>
        <div class="t">deep的network可以製造出比較多的片段,但是deep的network可以製造出比較多片段這件事情,跟我們要fit某一個function並沒有直接的相關。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:15.220" id=35:15.220>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2115">35:15.220</a></div>
        <div class="t">我們現在真正關心的問題是,如果用deep的structure來fit某一個function的話,會是什麼樣子?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:24.060" id=35:24.060>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2124">35:24.060</a></div>
        <div class="t">假設我們現在要fit的function是一個比較簡單的function,我們先從這個比較簡單的function開始討論。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:31.500" id=35:31.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2131">35:31.500</a></div>
        <div class="t">這個function是f of x等於x的平方,長得就是這個樣子,x平方在0到1之間的區間長得是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:43.700" id=35:43.700>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2143">35:43.700</a></div>
        <div class="t">現在就算是我們不管是用shallow的structure還是deep的structure,我們製造出來的function都是piecewise linear的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:52.420" id=35:52.420>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2152">35:52.420</a></div>
        <div class="t">所以在討論怎麼用一個shallow的network或者甚至是deep的network來fit某一個function之前,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#35:57.540" id=35:57.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2157">35:57.540</a></div>
        <div class="t">我們要討論的第一件事情都是怎麼用一個piecewise linear的function去fit我們現在的他天function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#36:06.100" id=36:06.100>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2166">36:06.100</a></div>
        <div class="t">我們現在的他天function是x平方,怎麼用一個piecewise linear的function去fit這個f of x等於x平方這個function呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#36:16.660" id=36:16.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2176">36:16.660</a></div>
        <div class="t">我們現在定義另外一個function叫做f小n of x,這個f小n of x總共會有2個n次方的片段,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#36:28.560" id=36:28.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2188">36:28.560</a></div>
        <div class="t">那f1 of x長得是這個樣子,這個f1怎麼來呢?你就取0.5的地方,然後跟頭接起來,跟尾巴接起來,就得到2個片段,所以今天f1總共有2的一次方,也就是2個片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#36:52.720" id=36:52.720>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2212">36:52.720</a></div>
        <div class="t">而接下來f2呢?f2有4個片段,這4個片段怎麼產生呢?你就把x軸,橫軸等於0到1⁴的地方接起來,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#37:07.440" id=37:07.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2227">37:07.440</a></div>
        <div class="t">x軸等於1⁴到1⁴的地方接起來,1⁴到3⁴的地方接起來,3⁴到1⁴的地方接起來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#37:15.200" id=37:15.200>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2235">37:15.200</a></div>
        <div class="t">我知道你在台下其實很難看得清楚這樣子,但是畫出來就是這個樣子我也沒有辦法。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#37:20.960" id=37:20.960>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2240">37:20.960</a></div>
        <div class="t">好,所以f2呢?它有4個片段,2次方總共4個片段,它是從0到1⁴,1⁴到2⁴,1⁴到3⁴,3⁴到1⁴,總共4個片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#37:36.880" id=37:36.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2256">37:36.880</a></div>
        <div class="t">那你會發現說其實f2跟x平方其實也蠻貼近的,所以你有點看不清楚f2在哪裡。那如果你畫f3的話,它就有8個片段,畫f4的話,它就有16個片段,以此類推。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#37:51.680" id=37:51.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2271">37:51.680</a></div>
        <div class="t">好,接下來我們問的問題是,這個n的值到底要到多少的時候?就是我們先給定這個ε,n的值要到多少的時候,我們才能夠讓fx跟fnx它們之間的最大的差小於等於ε?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#38:13.000" id=38:13.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2293">38:13.000</a></div>
        <div class="t">當然我們知道說,這個n越多,fn跟f之間的差異就越小。你這邊畫越多的片段,那你畫出做出來的fn跟f,也就是x平方它們就越接近。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#38:27.800" id=38:27.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2307">38:27.800</a></div>
        <div class="t">但是假設我們今天它們的差距不可以超過ε,要小於等於ε的話,這個n應該要有多少呢?你實際上算一下的話,這邊就省略掉計算過程,你就回去怒算一波這樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#38:43.320" id=38:43.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2323">38:43.320</a></div>
        <div class="t">如果我算出來有錯,你再告訴我這樣。你就回去怒算一波,你就會知道說,今天要如何讓n要多少,才能夠讓它們差異小於等於ε呢?這個n要大於等於-1 2 log2ε-1的時候,這個n就會小於等於ε。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#39:06.600" id=39:06.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2346">39:06.600</a></div>
        <div class="t">你說我怎麼算呢?你就把fn的式子列出來,然後你就可以算它跟這個x平方的差距。只是這個計算並不困難,它有點繁瑣,然後你就可以算出說,怎麼樣可以讓它們最大的差距小於等於ε。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#39:25.800" id=39:25.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2365">39:25.800</a></div>
        <div class="t">總之你算出來就是這個樣子。可是這邊有點負號,那是不是這一項是負的呢?不是,ε是小於1的,ε是一個很小的值,懂嗎?如果ε是一個很小的值,所以log2ε是負的,乘上負的東西是正的,再減掉1這樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#39:42.840" id=39:42.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2382">39:42.840</a></div>
        <div class="t">ε可能是一個很小的值,比如說2的負7次方,出來這邊就是變成負7,負7乘以1 2變成正的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#39:51.440" id=39:51.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2391">39:51.440</a></div>
        <div class="t">所以n要大於等於這個數值,才能夠讓它小於等於ε。當然如果今天ε的值越小,n就要越大,就這樣。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#40:01.440" id=40:01.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2401">40:01.440</a></div>
        <div class="t">n是這個樣子,那需要多少個片段呢?這邊取2的n次方,這邊取2的這項次方。所以你今天需要的片段的數目,至少要大於等於1⅔√ε1的片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#40:22.940" id=40:22.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2422">40:22.940</a></div>
        <div class="t">這個值怎麼來的呢?你就是把取二的這個次方,你就把這個東西放在那個指數下,然後二的這個次方就是二分之一,乙除以根號x了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#40:37.540" id=40:37.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2437">40:37.540</a></div>
        <div class="t">好,那講到這邊,假設你沒有跟上的話,你只要知道說,現在有一個x平方的function,我們今天如果要用二個m次方的一樣寬的片段去fit x平方這個function,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:02.740" id=41:02.740>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2462">41:02.740</a></div>
        <div class="t">那我們的這個二的m次方的數目,一定要大於二分之一,乙除以根號x的片段。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:11.340" id=41:11.340>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2471">41:11.340</a></div>
        <div class="t">那你會發現說呢,這個東西算起來,其實是比剛才我們在第一堂課裡面得到的結果,l除以x還要小的,對不對?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:22.340" id=41:22.340>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2482">41:22.340</a></div>
        <div class="t">你想想看,這邊它是除x0,這邊是除根號x0,所以如果你算它的這個O的話,這一下是比較小的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:34.940" id=41:34.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2494">41:34.940</a></div>
        <div class="t">那這個結果其實也是很合理的,因為在第一堂課我們討論的是general的case嘛,是任意的function,這邊我們只是討論x平方。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:43.240" id=41:43.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2503">41:43.240</a></div>
        <div class="t">那你需要比較少的,就是根據這個x平方的特性,所以你需要比較少的function就可以fit,你需要比較少的片段就可以fit它,這個結果也是頗為合理的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#41:54.340" id=41:54.340>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2514">41:54.340</a></div>
        <div class="t">好,那現在我們剛才講過說,假設是一個shallow network的話,你要兩個relu才能夠產生一個piece,但其實更少,有時候一個relu就可以產生一個piece。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#42:06.140" id=42:06.140>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2526">42:06.140</a></div>
        <div class="t">那無論如何,你需要的neuron的數目就是big O到1除以根號x0,你需要至少這麼多的neuron,就假設你是shallow的network,你需要至少這麼多的neuron才可以fit f of x等於x平方,這個是shallow的狀況。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#42:25.140" id=42:25.140>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2545">42:25.140</a></div>
        <div class="t">好,那我們來看一下deep的狀況。deep它的厲害的地方就是,它要製造出這麼多pieces,其實它並不需要這麼多的neuron。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#42:38.440" id=42:38.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2558">42:38.440</a></div>
        <div class="t">如果我們要用deep的network來產生f of x等於x平方,那要怎麼做呢?你看哦,你要產生f1的話,你要產生f1 of x,你其實只需要把一條斜率是1的斜線減掉,這一個東西它就是f1了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#43:01.440" id=43:01.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2581">43:01.440</a></div>
        <div class="t">這樣大家可以接受嗎?你想想看哦,這個是我們先把x平方畫出來,假設畫出來是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#43:24.440" id=43:24.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2604">43:24.440</a></div>
        <div class="t">好,把f1畫出來一下,假設f1畫出來是這個樣子,假設f1畫出來是這個樣子,畫綠色的好了,是這個樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#43:39.440" id=43:39.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2619">43:39.440</a></div>
        <div class="t">好,那f1跟這一個斜率是1的直線,它們中間這邊差多少呢?假設這邊是0.5的話,這邊的高是0.5的平方,是0.25。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#43:58.440" id=43:58.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2638">43:58.440</a></div>
        <div class="t">這邊的高,這是一個斜率是1的直線,這樣的話很不標準,但是這邊的高是0.5,所以這邊的差距是0.25。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#44:10.440" id=44:10.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2650">44:10.440</a></div>
        <div class="t">好,所以你把這一個斜線減掉這中間的差,這邊最高的地方是0.25,它後面會慢慢變小這樣,這邊是0,它頭尾的地方都是0,中間差距最大是0.25,就會變成這個f1,這一個piecewise裡面的方圈。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#44:34.440" id=44:34.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2674">44:34.440</a></div>
        <div class="t">所以大家可以接受嗎?所以其實你把這一個藍色的斜線減掉這一個三角形,這個三角形就是這邊這一塊,雖然看起來不像,但它就是啊,減掉這一塊現在塗顏色的地方,它就變成了f1這樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#44:56.440" id=44:56.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2696">44:56.440</a></div>
        <div class="t">所以我們這樣可以用這兩個方圈相減,製造出f1。怎麼製造出f2呢?你就再產生這樣子,這樣子,有兩個鋸齒的形狀,第一個鋸齒減在這邊,第二個鋸齒減在這邊,你就產生f2了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#45:21.440" id=45:21.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2721">45:21.440</a></div>
        <div class="t">這樣OK嗎?教練,大家有問題要問的嗎?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#45:31.440" id=45:31.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2731">45:31.440</a></div>
        <div class="t">這兩個三角形的高度是一樣的。你可以自己check一下他們的高度是一樣的。我們就不要畫圖好了,因為這個圖其實是很難畫的,你如果不相信就回去check看看這樣子。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#45:56.440" id=45:56.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2756">45:56.440</a></div>
        <div class="t">總之,你把這一個斜線減掉第一個三角形,你就得到藍色這一條線,然後再減掉第二個這樣子的兩個鋸齒的三角形,用滑鼠很難指,兩個鋸齒的三角形,你就得到f2。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#46:18.440" id=46:18.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2778">46:18.440</a></div>
        <div class="t">所以我們知道說,一個斜線減掉這個就是f1,一個斜線減掉這個再減掉這個就是f2。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#46:30.440" id=46:30.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2790">46:30.440</a></div>
        <div class="t">如果我們今天要產生fn的話,那我們知道n的值要大於等於這一項,如果我們要產生fn的話,那我們做的事情就是把這條斜線減掉一個三角形,再減兩個三角形,減四個三角形。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#46:48.440" id=46:48.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2808">46:48.440</a></div>
        <div class="t">不不不,對對對,沒錯沒錯,是一個三角形減掉一個三角形,兩個三角形,四個三角形,八個三角形,一直減下去,減掉直到減到二個n四方的三角形。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#47:00.440" id=47:00.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2820">47:00.440</a></div>
        <div class="t">他們的高度你要考慮一下,一個三角形的時候高度是四分之一,兩個三角形的時候高度是十六分之一,二個n四方三角形的時候高度是四的n四方分之一。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#47:18.440" id=47:18.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2838">47:18.440</a></div>
        <div class="t">那怎麼產生這一連串的三角形呢?其實就跟我們在上一堂課講的,我們不是說用這個V字形的取絕對值的neuron,其實就是兩個ReLU。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#47:31.440" id=47:31.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2851">47:31.440</a></div>
        <div class="t">我們在第一層就可以製造出一個V的形狀,可以把V的形狀乘上一個負號,其實就是一個三角形。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#47:39.440" id=47:39.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2859">47:39.440</a></div>
        <div class="t">在第二層你可以製造出一個W,把W的形狀乘一個負號,就是這個變成一個m的形狀,就變成兩個三角形了,一直到你總共有m層,到第m層的時候,你就可以製造這個有二個m四方的三角形的句詞狀的圖了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#47:56.440" id=47:56.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2876">47:56.440</a></div>
        <div class="t">所以今天假設你要製造這些function,其實你只需要一個有m層的ReLU的network,其實就可以辦到了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#48:10.440" id=48:10.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2890">48:10.440</a></div>
        <div class="t">所以如果我今天要製造這個Fm,其實我只需要產生這個東西,那這個東西很簡單,你even不需要activation function就可以製造,就input and output嘛,就是原來的x,然後再減掉a1,再減掉a2,再一直減到aN,只是你要注意一下這中間的scalar,你其實就製造出Fm了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#48:34.440" id=48:34.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2914">48:34.440</a></div>
        <div class="t">所以假設你要製造Fm的話,你只需要m個layer,你只需要big-old-of-m的neural,然後總是big-old-of-m個layer,就可以製造這種activation function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#48:48.440" id=48:48.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2928">48:48.440</a></div>
        <div class="t">如果你把這個m代-1 2 log2 epsilon-1的話,那這個-1 2,這個可以提到log裡面,變成1除以根號epsilon,所以你只需要log2 1除以根號epsilon的neural,然後log2 1除以根號epsilon的layer,其實就可以產生,就可以去逼近y等於x平方這樣子的target function了,就這樣。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#49:14.440" id=49:14.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2954">49:14.440</a></div>
        <div class="t">那你可能會想說,好像只討論了y等於x,假設剛才的東西沒跟上的話,你只要知道說,如果我們今天用deep的neural,因為deep的neural可以輕易地產生這種句詞的形狀,所以我們就可以輕易地fit y等於x平方,比general的用shallow的neural還要的neural要少很多。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#49:33.440" id=49:33.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2973">49:33.440</a></div>
        <div class="t">那為什麼我們在意y等於x平方呢?你可能會覺得說,只考慮y等於x平方感覺非常的limited,但其實不會,y等於x平方它有很多的妙用,怎麼用y等於x平方呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#49:47.440" id=49:47.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=2987">49:47.440</a></div>
        <div class="t">我們現在知道說,我們只要log2 1除以根號epsilon的neural,我們就可以製造一個square net,這個square net它做的事情不像現在Rnet都做一些很複雜的事情,它就是乘平方,然後它的誤差會小於等於epsilon。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#50:05.440" id=50:05.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3005">50:05.440</a></div>
        <div class="t">那我們能夠製造square net以後,我們就可以製造一種特殊的network叫做multiply net,它multiply net做的事情就是給它s1 s2,它給你output,把s1 s2相乘。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#50:17.440" id=50:17.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3017">50:17.440</a></div>
        <div class="t">怎麼從square net製造multiply net呢?因為y等於s1 s2,等於1 2s1加s2的平方,減s1平方減s2平方。你知道吧,s1加s2的平方展開,減s1平方減s2平方再除以1 2,你就得到了s1 s2。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#50:35.440" id=50:35.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3035">50:35.440</a></div>
        <div class="t">所以今天我們只要會做square net,你接下來就可以用三個square net拼出一個multiply net。怎麼拼呢?我們先把s1 s2加起來,丟到square net裡面,然後把s1獨自丟到square net裡面,把s2獨自丟到square net裡面。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#50:55.440" id=50:55.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3055">50:55.440</a></div>
        <div class="t">然後這邊就是算出s1加s2的平方,這邊算出s2平方,這邊算出s1平方的這三項,就是這邊的這三項,把它算出來都乘上1 2,加起來就得到s1 s2了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#51:10.440" id=51:10.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3070">51:10.440</a></div>
        <div class="t">所以今天我們能夠用這麼多的neuron做出square net,我們就是用三倍的neuron就可以做出multiply net了。那這個big O的complexity是不變的,因為只是在前面乘上一個常數而已。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#51:27.440" id=51:27.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3087">51:27.440</a></div>
        <div class="t">好,能夠做multiply net以後呢,接下來你就可以做polynomial,因為你就可以做,至少你就可以先做y等於xn方。怎麼做y等於xn方呢?很簡單,你就用一個square net把x變成x平方,接下來我們會做multiply net,你就可以把x平方跟x乘起來變成x三方,你還可以把x跟x三方乘起來就變成x四方。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#51:50.440" id=51:50.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3110">51:50.440</a></div>
        <div class="t">所以今天要產生xn方,沒有問題,你可以用一堆的multiply net就可以做到這些事情。但這不是唯一的方法,你永遠可以想一下別的方法,舉例來說,你也可以只用square net就算出x四方等等。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#52:04.440" id=52:04.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3124">52:04.440</a></div>
        <div class="t">總之,你有了square net,有了multiply net,你就可以算y等於x的n方,而這邊的每一個block需要的neuron的complexity是這麼多,是log1除以根號s。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#52:20.440" id=52:20.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3140">52:20.440</a></div>
        <div class="t">接下來你可以算y等於x平方,你就製造了一個東西叫做power net,後面有個n,就是它可以把input的x變成x的n方,我們叫它power net。而你有power net以後,你就可以產生polynomial了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#52:36.440" id=52:36.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3156">52:36.440</a></div>
        <div class="t">怎麼產生polynomial?你用power net產生xn方,前面代成an,然後你用power net產生xn-1方,前面代成an-1,把它統統加起來,你就可以產生任何你想要的polynomial了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#52:52.440" id=52:52.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3172">52:52.440</a></div>
        <div class="t">那你說polynomial不夠general,其實polynomial就夠general,因為可以用polynomial去fit其他的continuous的function就結束了。所以我們現在可以用deep的structure,我們會做x平方,x平方只要bit out of log1除以xn的neuron,然後接下來最後就可以產生polynomial,然後就可以用polynomial的function去fit其他的function。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#53:16.440" id=53:16.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3196">53:16.440</a></div>
        <div class="t">我們就知道說怎麼用deep的network架構去fit其他的function了。當然你可能會說,在實作上,這個network不是像你這樣手設出來的,那個是認出來的。我們現在還沒有討論那個問題,我們只是想討論說,假設你要fit某一個function,有沒有辦法做到。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#53:36.440" id=53:36.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3216">53:36.440</a></div>
        <div class="t">實際上你找不找得到那個function,那是optimization問題,那是我們之後才要討論的問題,不是我們今天要討論的問題。所以我們現在得到這樣的結果,我們要fit y等於x平方,如果是shallow的network,你的neuron的數目是bit out of e除以根號ε,那我就把e除以根號ε畫出來,橫軸是ε,然後把e除以根號ε的線畫出來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#54:02.440" id=54:02.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3242">54:02.440</a></div>
        <div class="t">那deep的network是log e除以根號ε,所以你發現說他們中間的差距是有一個log向的,他們中間的差距有exponential那麼多,他們是exponential的差距。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#54:15.440" id=54:15.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3255">54:15.440</a></div>
        <div class="t">deep它比shallow還要,你要達到同樣的error的時候,deep它需要的neuron是shallow需要的neuron再取log,或者是說deep可以用某個數量的neuron達成某個accuracy,那shallow的network要exponential多的neuron才可以達到那個accuracy。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#54:38.440" id=54:38.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3278">54:38.440</a></div>
        <div class="t">但是這樣子的討論是不足夠的,你覺得不足在哪裡呢?你仔細想想看,你回憶一下獵人第二十集比斯集告訴我們的,比斯集說,假設有很多個每個人的能力範圍其實都是一個range。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#54:57.440" id=54:57.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3297">54:57.440</a></div>
        <div class="t">今天如果我們拿C跟A來比較,然後A我們找它的一個case,正好找到它比如說狀況特別差的case,C我們找到一個case,正好找到一個狀況特別好的case,我們就會覺得說C可以贏過A,但其實那只是因為A正好選到狀況特別差的case,也許A在最佳狀態的時候它是可以達到C的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#55:19.440" id=55:19.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3319">55:19.440</a></div>
        <div class="t">所以今天的狀況也是一樣,我們剛才說shallow需要這麼多的neuron只是說我們想了一個方法需要這麼多的neuron,並不代表說那個是最佳的solution啊,對不對?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#55:29.940" id=55:29.940>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3329">55:29.940</a></div>
        <div class="t">我們只是說,如果有這麼多neuron,我可以fit y等於x²,但是並不代表說我一定要那麼多才能fit y等於x²,也許只要比較少的neuron就可以辦到這件事也說不定。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#55:43.040" id=55:43.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3343">55:43.040</a></div>
        <div class="t">也許這是shallow的neuron它在很糟的狀態,就是它其實是A,它其實在這邊,它很糟的狀態,也許它竭盡全力的時候它是這個樣子,它可以打爆deep,只是我們不知道而已。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#55:55.240" id=55:55.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=FN8jclCrqY0&t=3355">55:55.240</a></div>
        <div class="t">所以下一段要講的就是,假設我們現在讓shallow的neuron竭盡全力,它能不能夠打爆deep呢?</div>
    </div>
    
</body>
</html>   