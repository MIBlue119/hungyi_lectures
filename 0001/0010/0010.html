<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Hung-yi Lee's Lectures(台大李宏毅老師系列課程)</title>
    <style>
        body {
            font-family: sans-serif;
            font-size: 18px;
            color: #111;
            padding: 0 0 1em 0;
        }
        .l {
          color: #050;
        }
        .s {
            display: inline-block;
        }
        .e {
            display: inline-block;
        }
        .t {
            display: inline-block;
        }
    </style>
  </head>
  <body>
    <a href="../../index.html">back to index</a>
    <h2>【生成式AI】大模型 + 大資料 = 神奇結果？(1/3)：大模型的頓悟時刻</h2><a href=https://www.youtube.com/watch?v=SaZTJJNOCOY><img src=https://i.ytimg.com/vi_webp/SaZTJJNOCOY/hqdefault.webp></a><br>
    
    <div class="c">
        <a class="l" href="#00:00.720" id=00:00.720>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=0">00:00.720</a></div>
        <div class="t">現場的同學、線上的同學,大家好啊!</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:05.040" id=00:05.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=5">00:05.040</a></div>
        <div class="t">我們就來繼續講大型語言模型的故事。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:10.320" id=00:10.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=10">00:10.320</a></div>
        <div class="t">今天會講兩件事啦,我們講一下大模型跟大資料有什麼神奇的地方。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:17.120" id=00:17.120>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=17">00:17.120</a></div>
        <div class="t">然後呢,我們講一下GPT-4,大家知道說禮拜三的時候GPT-4釋出了,來講一下實測的結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:25.440" id=00:25.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=25">00:25.440</a></div>
        <div class="t">那今天這份投影片呢,是要講大模型跟大資料帶來什麼神奇的力量。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:34.240" id=00:34.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=34">00:34.240</a></div>
        <div class="t">左邊這個圖一樣是MeJourney生成的啦,那我這邊下的指令是說畫一個巨大的語言模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:42.000" id=00:42.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=42">00:42.000</a></div>
        <div class="t">這個Colossal是巨大的意思,你知道有個電影叫《科索羅巨獸》,科索羅就是非常大的意思。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:49.600" id=00:49.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=49">00:49.600</a></div>
        <div class="t">就畫一個巨大的語言模型,然後他要展現出不可思議的力量,MeJourney就畫一個這樣子的圖啦。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#00:57.040" id=00:57.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=57">00:57.040</a></div>
        <div class="t">他覺得他畫一個大怪獸啊,我不知道為什麼這個大怪獸算是一個language model,算是一個語言模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:03.280" id=01:03.280>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=63">01:03.280</a></div>
        <div class="t">然後我發現啊,你只要叫MeJourney畫大怪獸,他前面一定會畫一個小人,就不管你有沒有叫他畫人,他都要畫一個人。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:10.160" id=01:10.160>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=70">01:10.160</a></div>
        <div class="t">那我覺得這是要對比出這個怪獸有多麼的大這樣,怪獸相對於這個人而言他是非常的巨大的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:17.760" id=01:17.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=77">01:17.760</a></div>
        <div class="t">那你可能會覺得說,大模型、大資料本來就會比較好啊,那這個故事有什麼好講的呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:25.040" id=01:25.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=85">01:25.040</a></div>
        <div class="t">那確實大家都知道,大模型、大資料就等於非常厲害。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:30.240" id=01:30.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=90">01:30.240</a></div>
        <div class="t">那在20年的時候,這邊是引用一篇比較舊的論文,一篇paper,這個是OpenAI的,它的標題是Scaling Loss for Neural Language Model。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:40.960" id=01:40.960>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=100">01:40.960</a></div>
        <div class="t">那裡面就很明顯地展示告訴你說,大模型、多資料,結果就會好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#01:46.880" id=01:46.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=106">01:46.880</a></div>
        <div class="t">那以下是這篇比較古早的論文的一個實驗結果。這個縱軸是什麼呢?縱軸是說,當我們在訓練一個語言模型的時候,就是去做文字接龍。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:01.280" id=02:01.280>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=121">02:01.280</a></div>
        <div class="t">那這個縱軸呢,你可以想成是文字接龍的Loss,或者是文字接龍的錯誤率。如果你不知道Loss是什麼的話,就想成文字接龍的錯誤率,總之越低越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:13.760" id=02:13.760>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=133">02:13.760</a></div>
        <div class="t">當然我知道說,文字接龍的好壞不一定跟最後你要解的任務的效能是成正相關的,因為畢竟文字接龍是一回事。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:23.280" id=02:23.280>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=143">02:23.280</a></div>
        <div class="t">接下來老師的調教還有reinforcement learning的調教也是很重要的,所以文字接龍好並不代表最後在應用的時候會做得好。等一下我們還會再看到這個議題。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:34.800" id=02:34.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=154">02:34.800</a></div>
        <div class="t">我知道說,語言模型、文字接龍做得好,不一定在後端的應用會做得好。不過在這篇論文裡面是直接假設說,語言模型、文字接龍做得好,反正它就是好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#02:46.240" id=02:46.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=166">02:46.240</a></div>
        <div class="t">假設要把文字接龍這件事情做好,如果你的模型的參數量越來越多,左邊這張圖從左到右代表模型越來越大,那你就會發現說,你在做文字接龍的時候,預測下一個字的錯誤率就會越來越低。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:05.600" id=03:05.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=185">03:05.600</a></div>
        <div class="t">同樣的道理,看右邊這個圖,這個橫軸代表的是訓練的資料量。當我們訓練的資料量越來越多的時候,文字接龍的錯誤率也會越來越低。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:18.400" id=03:18.400>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=198">03:18.400</a></div>
        <div class="t">所以這個結果很直覺,大模型就是好,多資料就是好。這個課程差不多上到這邊,就可以下課了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:26.560" id=03:26.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=206">03:26.560</a></div>
        <div class="t">接下來我們要講講大模型還有什麼樣神奇的現象。大模型有一個神奇的現象,我叫做頓悟時刻。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:39.520" id=03:39.520>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=219">03:39.520</a></div>
        <div class="t">英文的話,論文上、文獻上都是講emergent ability,如果直接把emergent ability翻譯成中文,也許你可以翻成湧動現象。當模型越來越大的時候,語言模型會在某一個瞬間突然就頓悟。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#03:56.460" id=03:56.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=236">03:56.460</a></div>
        <div class="t">這個頓悟到底是什麼意思呢?我們來看文獻上面的實驗結果。在這篇論文裡面,就是嘗試讓各種不同大小的模型去解了八個不同的任務。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:10.040" id=04:10.040>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=250">04:10.040</a></div>
        <div class="t">這邊嘗試了很多不同的模型,包括Google的浪打、OpenAI的GPT-3、Google和Ginja都是D-MIC的,這個我們等一下還會提到,PALM也是Google的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:24.320" id=04:24.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=264">04:24.320</a></div>
        <div class="t">圖上的縱軸代表說這些模型在這些任務上,這邊有總共八個任務,在八個不同任務上的正確率,虛線代表隨機亂猜的結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:38.420" id=04:38.420>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=278">04:38.420</a></div>
        <div class="t">你會發現說當模型很小的時候,這邊模型是從10M到100個Billion的參數,當模型在10M到大概1個Billion參數的時候,在這些任務上它的程度都跟隨機差不多。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#04:55.360" id=04:55.360>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=295">04:55.360</a></div>
        <div class="t">而模型大概超過10個Billion的時候,它會在一瞬間突然頓悟,本來什麼都不會,本來模型從10M到1B並沒有慢慢變好。照理說你會覺得模型變大,它應該是稍微慢慢變強吧,從本來什麼都不會,到會一點點,到會更多,然後慢慢變強。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:14.420" id=05:14.420>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=314">05:14.420</a></div>
        <div class="t">但是這邊模型展現的能力並不是隨著模型變大而慢慢變強,而是有一個頓悟的瞬間,感覺模型大概在10B到20B的時候,它會突然開悟了,突然這些任務就都做得起來了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:28.660" id=05:28.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=328">05:28.660</a></div>
        <div class="t">在這之前,就算模型加大,你做起來都是跟隨機的結果差不多。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:34.880" id=05:34.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=334">05:34.880</a></div>
        <div class="t">所以這是一個神秘的頓悟現象,這邊其實造成你在開發這個技術的時候你會有一個隱憂,因為你一開始一定是先從小的模型開始勸起。你先勸個10M的模型覺得很差,那就想說加大一點點吧,加到1個Billion的參數看看會怎樣,然後還是很差。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#05:52.660" id=05:52.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=352">05:52.660</a></div>
        <div class="t">那你跟老闆說,這個1Billion的模型還是很差,能不能勸個再更大一點的?老闆就會跟你說,10M到1Billion都沒有進步了,再更大怎麼會有進步呢?就告訴你這個project,你要放棄,不要浪費公司的錢。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:05.300" id=06:05.300>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=365">06:05.300</a></div>
        <div class="t">所以這個頓悟的現象,其實也許就是我們在開發大型語言模型的時候會遇到的一個難關。它會讓你沒有辦法說服你的投資人去投資一個更大的模型,因為模型慢慢變大的時候並不會變好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:22.640" id=06:22.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=382">06:22.640</a></div>
        <div class="t">要大到一個程度才會突然變好。那為什麼會有這個頓悟的時刻呢?我們來舉這個解數學的例子。對小模型來說,你叫它解一個數學的問題,因為它很笨,所以它什麼都不會,所以它沒列式子也解不出答案,所以直接就得到0分。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:40.900" id=06:40.900>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=400">06:40.900</a></div>
        <div class="t">那對一個中的模型呢?也許中的模型它公式列對了,但是最後計算錯誤了。這個跟安琪拉小分隊一樣,公式對了,但是代數錯了。沒人聽得懂,算了,這真不重要,這梗不重要,這真還賺的梗,這真的不重要。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#06:58.000" id=06:58.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=418">06:58.000</a></div>
        <div class="t">公式對了,計算錯誤,那一樣是0分。數學的考題我們都只看最後的答案,不看你列的過程。大模型,公式對了,計算也對了,才有100分。所以如果你從小模型一直看到大模型,你會發現從0分、0分,然後在某一瞬間突然變成100分,就好像開悟了一樣。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:17.620" id=07:17.620>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=437">07:17.620</a></div>
        <div class="t">所以這個就是大型模型的頓悟時刻。從這個頓悟時刻給我們的啟示應該是說,我們在分析這個模型效能的時候不要只看正確率,你只要解數學問題的時候不要只看對或錯,你其實可以看一下它列的過程。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:34.320" id=07:34.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=454">07:34.320</a></div>
        <div class="t">這樣也許可以給你更多的信心,也許你在從小模型開發到中模型的時候發現,它公式其實列對了,只差一點點計算,只要不要出錯,它答案就會對了,也許會給你一些信心,把模型從中模型變成大模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#07:48.840" id=07:48.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=468">07:48.840</a></div>
        <div class="t">這個頓悟的時刻不是只在某一些task上發現,在一些我們今天已知的跟這個quantum大模型有關的技巧,也跟模型的大小很有關係。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:03.500" id=08:03.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=483">08:03.500</a></div>
        <div class="t">記不記得我們上週講了chain of sort,我們講chain of sort的時候說,你只要告訴模型你在想問題的時候要一步一步慢慢來,它今天解數學的能力就突然進入另外一個量級。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:16.680" id=08:16.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=496">08:16.680</a></div>
        <div class="t">但是chain of sort這個現象是要大模型才會發生的。我們來看最左邊這個圖,縱軸是解數學問題的正確率,橫軸是模型的參數量從小到大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:30.440" id=08:30.440>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=510">08:30.440</a></div>
        <div class="t">如果不做chain of sort的時候,模型的參數量從小到大,它的進步是一點點。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:35.820" id=08:35.820>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=515">08:35.820</a></div>
        <div class="t">如果做chain of sort的時候,在模型小的時候,模型從1B到10B的時候,有做chain of sort其實是比較差的,你叫模型think step by step。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:46.220" id=08:46.220>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=526">08:46.220</a></div>
        <div class="t">其實它結果是比較差的,它是要到某一個瞬間以後,模型夠大以後,chain of sort才會突然發揮效用。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#08:54.660" id=08:54.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=534">08:54.660</a></div>
        <div class="t">那instruction tuning也是一樣,記不記得我們上週也講了instruction tuning,說你找了一堆任務讓機器去學,希望它看了在某一些任務上面,學了人類會用什麼樣的指令操控它以後,在新的任務上面也可以有好的結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:10.840" id=09:10.840>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=550">09:10.840</a></div>
        <div class="t">那instruction tuning這件事情,黑色這條線是沒有做instruction tuning的時候的正確率,沒有做instruction tuning的時候,模型隨著越來越大,它會慢慢地進步,但進步幅度是小的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:23.060" id=09:23.060>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=563">09:23.060</a></div>
        <div class="t">有做instruction tuning的時候,在模型小的時候,instruction tuning根本沒有發揮作用,模型要大到一定的程度以後,instruction tuning才突然之間會起作用。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:35.300" id=09:35.300>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=575">09:35.300</a></div>
        <div class="t">那當然現在這個也不難理解,因為chain of sort、instruction tuning都是給了模型額外的指示,比如說chain of sort就是要模型去think step by step,那小模型它很弱啊,你叫它think step by step,</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:48.800" id=09:48.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=588">09:48.800</a></div>
        <div class="t">它也可能只是產生一些隨機的結果,然後反而把模型自己搞得非常困惑,所以會變成有加chain of sort,小模型反而比較差。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#09:57.420" id=09:57.420>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=597">09:57.420</a></div>
        <div class="t">而chain of sort這件事情,要等模型夠大,你叫它think step by step的時候,它可以產生有道理的東西,chain of sort才開始發揮作用。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:06.800" id=10:06.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=606">10:06.800</a></div>
        <div class="t">當然如同我剛才講的,這種突然之間模型會開悟的現象,會造成你開發的時候非常大的危機。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:14.240" id=10:14.240>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=614">10:14.240</a></div>
        <div class="t">因為可能有人覺得說,instruction tuning應該是有用的方法,chain of sort應該是有用的方法,但一開始它開發在小模型上,然後就覺得instruction tuning應該沒有什麼作用,就會阻止你去使用這個技術。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:27.620" id=10:27.620>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=627">10:27.620</a></div>
        <div class="t">因為你可能在小模型的時候發現instruction tuning沒有作用,你就放棄了,你就不會想要去嘗試更大的模型了。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:34.600" id=10:34.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=634">10:34.600</a></div>
        <div class="t">右邊這兩個圖是什麼意思呢?這第三個圖,我把論文的連結放在這邊,這邊有一個方法叫做scratchpad,它其實跟chain of sort是差不多的意思。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:47.320" id=10:47.320>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=647">10:47.320</a></div>
        <div class="t">scratchpad就是說給機器一個白板,然後讓它在上面解數學的時候把數學式列出來,其實跟chain of sort的意思是差不多的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#10:57.200" id=10:57.200>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=657">10:57.200</a></div>
        <div class="t">最右邊這個我們可以特別講一下,calibration是什麼意思呢?大家都知道說現在chain GBT很多時候它會亂講話,舉例來說,你叫它介紹台大杜鵑花節,它會好好的接受。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:12.700" id=11:12.700>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=672">11:12.700</a></div>
        <div class="t">但是你叫它介紹台大玫瑰花節的時候,它也會煞有其事地講一個玫瑰花節的故事給你聽。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:20.080" id=11:20.080>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=680">11:20.080</a></div>
        <div class="t">但是我們這邊要問的事情是,模型知不知道自己在瞎掰?它在瞎掰一個答案的時候,有沒有覺得有一點心虛呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:29.640" id=11:29.640>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=689">11:29.640</a></div>
        <div class="t">怎麼知道模型有沒有在心虛呢?就是看它在輸出這個文字的時候,它輸出文字的機率。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:37.740" id=11:37.740>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=697">11:37.740</a></div>
        <div class="t">舉例來說,假設台大杜鵑花節是每年三月這件事情是一個事實,也許機器在預測三這個字的時候,它的confidence是很高的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:48.600" id=11:48.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=708">11:48.600</a></div>
        <div class="t">我這邊用不同高低的bar來代表說,三這個字機器做文字接龍的時候預測的機率,這個機率就代表一個信心分數。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#11:59.880" id=11:59.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=719">11:59.880</a></div>
        <div class="t">機率越高,代表機器對自己的答案是越有信心的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:03.980" id=12:03.980>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=723">12:03.980</a></div>
        <div class="t">可能機器在講台大杜鵑花節是每年三月的三的時候,它是非常有信心的,在講台大玫瑰花節是每年三月的三的時候,也許它的信心是很低的,只是在做random sample的時候,正好把三這個字sample出來而已。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:19.660" id=12:19.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=739">12:19.660</a></div>
        <div class="t">如果我們來分析機器答題的正確率跟它信心之間的關係的話,在左上角引用的這篇paper裡面得到這樣的一個實驗結果。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:33.860" id=12:33.860>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=753">12:33.860</a></div>
        <div class="t">橫軸代表機器在答題的時候,它答題的信心分數,這邊所謂的信心分數就是它伸出那個答案的時候,那個答案的機率,你可以算出那個答案產生出來的機率。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:46.980" id=12:46.980>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=766">12:46.980</a></div>
        <div class="t">縱軸是什麼?縱軸是指那個答案真正是對的的時候的機率有多大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#12:56.880" id=12:56.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=776">12:56.880</a></div>
        <div class="t">那你就可以看到說,這邊從黃色到深藍色指的是不同大小的模型,黃色是最大的模型,深藍色是最小的模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:07.560" id=13:07.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=787">13:07.560</a></div>
        <div class="t">你會發現最小的模型,它的信心分數跟它答案正確的機率關係不大,但是對於大模型而言,它的信心分數越高,它的正確率就越高,信心分數越低,它的正確率就越低。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:23.540" id=13:23.540>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=803">13:23.540</a></div>
        <div class="t">所以你發現,大模型它知道自己不知道,大模型在瞎掰一個答案的時候,它是有點心虛的,對小模型來說,不管是不是在瞎掰,它也不在乎,對它來說有沒有瞎掰的時候,它的信心的程度都是差不多的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:38.380" id=13:38.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=818">13:38.380</a></div>
        <div class="t">這個指的就是calibration,而要模型夠大的時候,它才具有calibration的能力。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#13:46.080" id=13:46.080>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=826">13:46.080</a></div>
        <div class="t">比如說我們看右邊這個圖,我們看藍色這一條線,這個橫軸是模型由小到大,縱軸是什麼?縱軸這邊說有一個東西叫做ECE,ECE指的是calibration的程度,ECE這個值是越小越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:04.740" id=14:04.740>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=844">14:04.740</a></div>
        <div class="t">ECE這個值其實就是左邊這張圖的對角線跟我們畫的這一條曲線之間所夾的面積。所以黃色這一條線,ECE算起來就非常小,黑色這一條線,ECE算起來就非常大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:23.000" id=14:23.000>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=863">14:23.000</a></div>
        <div class="t">因為深藍色這一條線跟橫軸、對角線的差距非常大,所以中間夾的面積很大,ECE算起來就很大。你會發現calibration這個現象在小模型到10B的時候,似乎不太有calibration的現象。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:38.600" id=14:38.600>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=878">14:38.600</a></div>
        <div class="t">但是到某一個瞬間,它突然知道說有時候它會講錯話,有時候它講錯話就會開始突然覺得很心虛,這也是要模型夠大才會突然發生的現象。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#14:49.460" id=14:49.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=889">14:49.460</a></div>
        <div class="t">到目前為止我們看到的,雖然說模型有一個頓悟的瞬間,但是畢竟模型越大,結果還是會越來越好,只是說在某一個瞬間會突然變得很好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:03.460" id=15:03.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=903">15:03.460</a></div>
        <div class="t">那有沒有一個可能是其實模型越大,結果越差呢?有一個比賽叫做Inverse Scaling Prize,這個比賽就是懸賞會讓越大模型做得很差的任務。這個是比賽的一個圖,它就說一般正常的狀況下,我們都是覺得模型越大,結果越好。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:26.180" id=15:26.180>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=926">15:26.180</a></div>
        <div class="t">但是這個比賽就是要徵求你能不能夠設計出一些奇怪的任務是模型越大,結果越差的呢?這個比賽是有獎金的,這個比賽叫Inverse Scaling Prize。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:38.740" id=15:38.740>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=938">15:38.740</a></div>
        <div class="t">因為有獎金,確實徵求到了很多奇奇怪怪的任務。這些奇奇怪怪的任務,這邊就舉幾個例子,等一下會帶大家看一個神秘的例子。徵求到一些任務,這些任務是模型越大,居然做出來結果越差。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#15:56.180" id=15:56.180>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=956">15:56.180</a></div>
        <div class="t">怎麼會這樣呢?後來就有一篇Google的paper,它說當年在做這個Inverse Scaling Prize的時候,大家試的都是一些小模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:09.300" id=16:09.300>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=969">16:09.300</a></div>
        <div class="t">它覺得GPT-3算是一個不夠大的模型,只有175個Billion,然後Google就發狠拿出PALM,有GPT-3的債,三倍大,然後來打這個Inverse Scaling Prize,看看會發生什麼事,來打這個Inverse Scaling Prize得到的任務看看發生什麼事。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:27.460" id=16:27.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=987">16:27.460</a></div>
        <div class="t">它的結果是這樣子的。縱軸是在那一些任務上,我們說大模型會做得比較差的任務上平均的正確率,橫軸是模型從小到大。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:41.060" id=16:41.060>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1001">16:41.060</a></div>
        <div class="t">這邊就是有各式各樣不同的模型了,Google PALM其實也有不同大小的版本,從小的一直到最大的有540個Billion的參數。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#16:51.780" id=16:51.780>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1011">16:51.780</a></div>
        <div class="t">他們發現一個神奇的shape,一個U型的shape,也就是說最小的模型對這些任務來說performance還比較好,比較大的模型結果反而比較差,到最大的模型的時候結果又好起來。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:05.220" id=17:05.220>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1025">17:05.220</a></div>
        <div class="t">所以他們說,之前在Inverse Scaling Prize找出來的那一些任務號稱會讓大模型變差,只是因為這些人試的模型不夠大,拿一個真正大的模型,結果還是會好起來的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:18.900" id=17:18.900>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1038">17:18.900</a></div>
        <div class="t">那接下來要問的就是,這個神秘的U型曲線是怎麼出現的呢?</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:29.860" id=17:29.860>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1049">17:29.860</a></div>
        <div class="t">那我們就來看Inverse Scaling Prize裡面的其中一個任務吧,這個任務的輸入在這邊。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:38.420" id=17:38.420>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1058">17:38.420</a></div>
        <div class="t">那我幫大家做了一下中文的翻譯啦,免得大家覺得英文字太小,我們講得很痛苦。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:43.860" id=17:43.860>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1063">17:43.860</a></div>
        <div class="t">這個問題是這樣,現在有一個賭局,這個賭局你有94%會輸50塊,你有6%會贏5塊。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#17:54.580" id=17:54.580>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1074">17:54.580</a></div>
        <div class="t">那David決定要玩這個遊戲,玩了以後贏了5塊錢,問你說這個賭局是不是一個正確的決定。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:04.660" id=18:04.660>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1084">18:04.660</a></div>
        <div class="t">我這邊做了一個失誤,就是我不小心把答案顯現出來了。但是這個答案也不一定是對的,那我問一下大家的意見吧。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:13.860" id=18:13.860>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1093">18:13.860</a></div>
        <div class="t">你覺得參加這個賭局是正確的,同學舉手一下。有同學覺得是正確的,那你覺得這個賭局是錯誤的,同學舉手一下。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:25.700" id=18:25.700>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1105">18:25.700</a></div>
        <div class="t">好,請放下,因為這個答案已經顯示在這邊了。這個答案是錯誤的。為什麼是錯誤的呢?因為雖然最後贏了5元,好像這個賭局是一個正確的決定。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:38.020" id=18:38.020>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1118">18:38.020</a></div>
        <div class="t">但是如果你算賭局的期望值的話,你會發現這個賭局的期望值是會虧錢的。所以最終的答案是,參加這個賭局不是一個正確的決定。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:50.500" id=18:50.500>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1130">18:50.500</a></div>
        <div class="t">那如果是對一個最小的模型來說,這個題目它可能根本看不懂題目,就隨便亂猜,所以它的程度就跟亂猜是一樣的。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#18:59.460" id=18:59.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1139">18:59.460</a></div>
        <div class="t">而對於一個中模型來說,它會說,贏了5元啊,贏了5元那不是就是一個正確的決定嗎?所以它會猜正確,結果反而比隨機還要差。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:11.460" id=19:11.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1151">19:11.460</a></div>
        <div class="t">如果大模型有機會計算期望值,它才能夠得到正確的答案。我們說大模型有可能會比較差的那些任務,基本上它的設計都很類似這個概念。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:25.380" id=19:25.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1165">19:25.380</a></div>
        <div class="t">這個表格是來自於這篇論文裡面,它就是稍微猜測一下,為什麼這個U型曲線會出現。它發現,那些會讓大模型比較差的任務,其實基本上這個任務裡面都包含了一個陷阱,它這邊叫做distracted task。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:43.380" id=19:43.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1183">19:43.380</a></div>
        <div class="t">你有一個真正的任務,但是還有一個陷阱任務,就好像你要讓一個人中計中計,你必須要讓對方看得出是一個計謀,他才會中計中計。如果他看不出是一個計謀,就中不了計中計。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#19:55.520" id=19:55.520>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1195">19:55.520</a></div>
        <div class="t">舉例來說,這邊有一個任務叫做redefine math,重新定義數學問題。裡面的題型都是這樣,假設π等於10,然後問你π加1等於多少,當然答案是11嘛。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:07.560" id=20:07.560>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1207">20:07.560</a></div>
        <div class="t">但是對一個中模型來說,它心理的認知就是π等於3.14,所以它認為3.14加1應該是4.14,所以它答不出正確答案來,它沒有辦法根據題目的敘述重新定義這些符號。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:20.460" id=20:20.460>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1220">20:20.460</a></div>
        <div class="t">模型要到真的很大的時候,它有辦法看懂π現在等於10,不是3.14,它才有辦法在這些會讓比較大的模型結果差的任務裡面得到真正好的結果。這個就叫做一枝半截吃大虧了,你有一點點的能力,然後反而會做得比較差。這個就是為什麼會有U型曲線出現的原因。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#20:47.800" id=20:47.800>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1247">20:47.800</a></div>
        <div class="t">接下來要問的是,模型還能不能夠更大呢?我們剛剛已經看到了很多大小的模型,從BERT、GPT-2、GPT-3、Lambda、一直到PALM有540個B連的參數。還有沒有更大的模型呢?還有更大的模型。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:07.380" id=21:07.380>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1267">21:07.380</a></div>
        <div class="t">舉例來說,有一個模型叫做Switch Transformer,它居然有1.6T的參數,這樣它就比PALM還要大3倍,比GPT-3還要大10倍。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:22.400" id=21:22.400>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1282">21:22.400</a></div>
        <div class="t">但像Switch Transformer這種模型,因為它實在是太大了,所以它的結構就跟原來的Transformer還是有一些差異。比如說Switch Transformer裡面有什麼樣的差異呢?它用了一種特殊的結構叫做Mixture of Experts。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:39.300" id=21:39.300>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1299">21:39.300</a></div>
        <div class="t">Mixture of Experts的意思是說,在它的大模型裡面其實有很多的小模組,那你今天在用這個模型的時候,不是一次用全部的模組,你只每次調用一部分的模組出來使用。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#21:55.880" id=21:55.880>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1315">21:55.880</a></div>
        <div class="t">就像在這個圖上,它可能展示說,它有四個模組,那它這邊只調用了模組二,在這邊只調用了模組一,不一次用所有的參數,不一次用所有的模組。這樣的好處是,它在Inference的時候比較快,它不需要所有的模組都使用。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:13.480" id=22:13.480>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1333">22:13.480</a></div>
        <div class="t">所以它在真的用這個模型的時候,並沒有真的用到所有的參數。它訓練的時候,訓練到所有的參數,但是在Inference的時候,在用的時候,只用了部分的參數,可以節省你Inference的時候需要的運算資源。</div>
    </div>
    
    <div class="c">
        <a class="l" href="#22:28.680" id=22:28.680>link</a> |
        <div class="s"><a href="https://www.youtube.com/watch?v=SaZTJJNOCOY&t=1348">22:28.680</a></div>
        <div class="t">這個是Mixture of Experts的概念。至於實際上怎麼訓練這種有多個不同的模組,然後在Inference的時候可以選擇模組的模型,我把論文連結列在這邊,留給大家自己參考。</div>
    </div>
    
</body>
</html>   